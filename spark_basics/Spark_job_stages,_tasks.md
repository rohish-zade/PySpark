## Job, Stage, and Task in Apache Spark

In Apache Spark, the concepts of Job, Stage, and Task are fundamental to understanding how Spark processes data in a distributed environment. 

These terms relate to how Spark breaks down a complex computation into smaller, manageable units that can be executed efficiently across a cluster.

  ![](https://github.com/rohish-zade/PySpark/blob/main/materials/Job%2C_Stage_Task.png)

### Potential interview question:

1️⃣ What is a job, stage and task in spark?

2️⃣ How many jobs will be created in the given question?

3️⃣ How many stages will be created?

4️⃣ How many tasks will be created?

## Job
