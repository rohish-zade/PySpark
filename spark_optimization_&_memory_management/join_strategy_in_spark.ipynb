{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df1fc276-5a44-42c0-ae3e-9f8fd7d9048a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Join Strategy in Spark\n",
    "\n",
    "In Apache Spark, joins are a critical operation for combining data from two or more datasets based on a related column. \n",
    "\n",
    "Spark provides several types of join strategies, and understanding these strategies can help you optimize performance and resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baef2b60-5f3c-495f-bf5d-a8e502711744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432c1b92-3212-48a1-a1ca-c92d50e6f6e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Potential Interview questions:**\n",
    "- What are the join strategies in Spark?\n",
    "- Why is join expensive/wide dependency transformations?\n",
    "- Difference between shuffle hash join and shuffle sort-merge join?\n",
    "- When do we need broadcast join?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b714cebb-b07e-4e6f-b86d-6230750bc680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f22ac88-bc5d-4bec-aee5-9bb5b7d8c582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Broadcast**\n",
    "- The driver will collect all the data partitions for table A, and broadcast the entire table A to all of the executors. This avoids an all-to-all communication (shuffle) between the executors which is expensive. \n",
    "- The data partitions for table B do not need to move unless explicitly told to do so.\n",
    "\n",
    "**Shuffle**\n",
    "- Shuffling in Spark refers to the process of redistributing data across the cluster, typically required when transformations like groupByKey, reduceByKey, join, or distinct are performed. \n",
    "- During shuffling, Spark moves data between partitions and worker nodes to ensure that related data ends up in the same partition for further computation. This process is essential but can be expensive in terms of performance.\n",
    "- All the executors will communicate with every other executor to share the data partition of table A and table B. All the records with the same join keys will then be in the same executors. \n",
    "- As this is an all-to-all communication, this is expensive as the network can become congested and involve I/O. After the shuffle, each executor holds data partitions (with the same join key) of table A and table B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb888cab-1735-4ff9-a3d9-224b6894bdbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51c17bb-7e04-465c-99d2-d2a74ad85bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Key join strategies in Spark:\n",
    "- Shuffle sort-merge join\n",
    "- Shuffle hash join\n",
    "- Broadcast hash join\n",
    "- Cartesian join\n",
    "- Broadcast nested loop join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d85a3fc-66fd-4c09-9d00-3bc785ff24cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Shuffle sort-merge join:\n",
    "- The Shuffle Sort-Merge Join is Spark’s default join strategy for large datasets in an equi-join scenario.\n",
    "\n",
    "- **How it works:**\n",
    "  - Spark shuffles data from both datasets based on the join key to ensure matching keys are colocated on the same node.\n",
    "  - Each partition of the shuffled data is then sorted by the join key.\n",
    "  - Finally, a merge operation is performed on the sorted data to produce the joined result.\n",
    "- **When to use:**\n",
    "  - Best for large datasets where broadcast join is not possible.\n",
    "  - Suitable for equi-joins\n",
    "- **Advantages:**\n",
    "  - Scales well for very large datasets.\n",
    "  - Does not require datasets to fit in memory.\n",
    "- **Disadvantages:**\n",
    "  - Expensive due to shuffling and sorting.\n",
    "\n",
    "**NOTE:**\n",
    "- This is Spark's default join for large tables when no broadcast is applicable.\n",
    "- You can disable it using **`spark.sql.join.preferSortMergeJoin = false.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "417fa6b0-254a-4042-8bed-ad7ae24f9aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#40L, name#41, dept#45]\n",
      "   +- SortMergeJoin [id#40L], [id#44L], Inner\n",
      "      :- Sort [id#40L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#40L, 200), ENSURE_REQUIREMENTS, [plan_id=168]\n",
      "      :     +- Filter isnotnull(id#40L)\n",
      "      :        +- Scan ExistingRDD[id#40L,name#41]\n",
      "      +- Sort [id#44L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(id#44L, 200), ENSURE_REQUIREMENTS, [plan_id=169]\n",
      "            +- Filter isnotnull(id#44L)\n",
      "               +- Scan ExistingRDD[id#44L,dept#45]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example in PySpark:\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")], [\"id\", \"name\"])\n",
    "df2 = spark.createDataFrame([(1, \"HR\"), (2, \"Engineering\"), (3, \"Sales\")], [\"id\", \"dept\"])\n",
    "\n",
    "# Shuffle Sort-Merge Join (default for large equi-joins)\n",
    "result = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59d734f9-ac3e-4514-8137-f41d1195780b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42f97979-0b51-4177-9ba9-264fe861f832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Shuffle hash join:\n",
    "- A join strategy that uses hashing instead of sorting.\n",
    "\n",
    "- **How it works:**\n",
    "  - Both datasets are shuffled by the join key.\n",
    "  - A hash table is created in memory for one of the datasets usually for the smaller one.\n",
    "  - Matching rows are found by probing the hash table.\n",
    "\n",
    "- **When to use:**\n",
    "  - Large datasets when you want to avoid sorting (e.g., when the data is not pre-sorted).\n",
    "\n",
    "- **Advantages:**\n",
    "  - Faster than sort-merge for smaller partitions.\n",
    "  - Avoids the sorting overhead.\n",
    "  \n",
    "- **Disadvantages:**\n",
    "  - Memory-intensive because it builds hash tables.\n",
    "  - Requires shuffling, which can be costly.\n",
    "\n",
    "\n",
    "**NOTE:**\n",
    "- Can be more efficient than sort-merge for certain data distributions.\n",
    "- Controlled by the configuration `spark.sql.join.preferSortMergeJoin.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33aadca7-d514-42c8-a766-554e0f87d667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#74L, name#75, dept#79]\n",
      "   +- SortMergeJoin [id#74L], [id#78L], Inner\n",
      "      :- Sort [id#74L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#74L, 200), ENSURE_REQUIREMENTS, [plan_id=408]\n",
      "      :     +- Filter isnotnull(id#74L)\n",
      "      :        +- Scan ExistingRDD[id#74L,name#75]\n",
      "      +- Sort [id#78L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(id#78L, 200), ENSURE_REQUIREMENTS, [plan_id=409]\n",
      "            +- Filter isnotnull(id#78L)\n",
      "               +- Scan ExistingRDD[id#78L,dept#79]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example in PySpark:\n",
    "\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")  # Prefer Hash Join\n",
    "\n",
    "result = df1.join(df2, on=\"id\", how=\"inner\")  # Shuffle Hash Join\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab44dc6-2ad7-4ba3-a8b9-b6acab1b518f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aba1614-0bad-4f5d-a10e-d8505b7061d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Broadcast hash join\n",
    "- A highly optimized join strategy where the smaller dataset is broadcast to all nodes.\n",
    "\n",
    "- **How it works:**\n",
    "  - The smaller dataset is broadcast (replicated) to all nodes in the cluster.\n",
    "  - Each node joins its local partition of the larger dataset with the broadcasted dataset.\n",
    "- **When to use:**\n",
    "  - When one dataset is small enough to fit in memory.\n",
    "  - The default threshold for broadcast is 10 MB, but this can be configured.\n",
    "- **Advantages:**\n",
    "  - Eliminates shuffling of the larger dataset.\n",
    "  - Extremely fast for small-large table joins.\n",
    "- **Disadvantages:**\n",
    "  - Limited by the size of the smaller dataset.\n",
    "  - Out-of-memory errors if the broadcasted dataset is too large.\n",
    "\n",
    "**NOTE:**\n",
    "- This is the fastest join for small-large datasets.\n",
    "- The broadcast size threshold is controlled by `spark.sql.autoBroadcastJoinThreshold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787159c8-521e-49dc-89bb-66e53cbbb01d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#74L, name#75, dept#101]\n",
      "   +- BroadcastHashJoin [id#74L], [id#100L], Inner, BuildRight, false, true\n",
      "      :- Filter isnotnull(id#74L)\n",
      "      :  +- Scan ExistingRDD[id#74L,name#75]\n",
      "      +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=489]\n",
      "         +- Filter isnotnull(id#100L)\n",
      "            +- Scan ExistingRDD[id#100L,dept#101]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example in PySpark:\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "small_df = spark.createDataFrame([(1, \"HR\"), (2, \"Engineering\")], [\"id\", \"dept\"])\n",
    "result = df1.join(broadcast(small_df), on=\"id\", how=\"inner\")\n",
    "result.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6968c815-9a97-492c-8ce4-9271d2c801f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fa23cd0-fd98-449b-8456-89cf61875c04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cartesian Join:\n",
    "- A join that produces a cross-product of two datasets.\n",
    "\n",
    "- **How it works:**\n",
    "  - Every row from the first dataset is paired with every row from the second dataset.\n",
    "  - This results in a dataset of size `n × m` (where `n` and `m` are the sizes of the input datasets).\n",
    "- **When to use:**\n",
    "  - When a cross-product is explicitly required, such as for combinatorial computations.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Can handle cases where no join keys are available.\n",
    "- **Disadvantages:**\n",
    "  - Extremely expensive for large datasets.\n",
    "  - Easily produces massive datasets that are difficult to process.\n",
    "\n",
    "**NOTE:**\n",
    "- Rarely used due to its computational cost.\n",
    "- Must be used with caution, especially for large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2bde7f0-667b-4a78-bcf5-13c1e83c842a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n",
      "| id| name|       dept|\n",
      "+---+-----+-----------+\n",
      "|  1|Alice|         HR|\n",
      "|  1|Alice|Engineering|\n",
      "|  2|  Bob|         HR|\n",
      "|  2|  Bob|Engineering|\n",
      "+---+-----+-----------+\n",
      "\n",
      "== Physical Plan ==\n",
      "CartesianProduct\n",
      ":- *(1) Scan ExistingRDD[id#107L,name#108]\n",
      "+- *(2) Scan ExistingRDD[dept#111]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example in PySpark:\n",
    "df1 = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df2 = spark.createDataFrame([(\"HR\",), (\"Engineering\",)], [\"dept\"])\n",
    "\n",
    "result = df1.crossJoin(df2)\n",
    "result.show()\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afa7b8e4-55a7-4d96-9347-7ca65012cec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392c304f-6e11-464f-b7f4-36f84248b97a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Broadcast Nested Loop Join\n",
    "- A join strategy for non-equi joins or when no join key is specified, but one dataset is small enough to be broadcast.\n",
    "\n",
    "- **How it works:**\n",
    "  - The smaller dataset is broadcast to all nodes.\n",
    "  - A nested loop iterates over each row of the larger dataset and matches rows based on the condition\n",
    "\n",
    "- **When to use:**\n",
    "  - For non-equi joins (e.g., range joins like `A.id < B.id`).\n",
    "  - When no natural join key is available, but one dataset is small.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Allows for flexible join conditions.\n",
    "  - Works with small datasets.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - Computationally expensive due to nested iteration.\n",
    "  - Not suitable for large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9a985d-df8c-4669-a876-3e047ce7f5ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----------+\n",
      "| id| name| id|       dept|\n",
      "+---+-----+---+-----------+\n",
      "|  1|Alice|  2|Engineering|\n",
      "|  2|  Bob|  1|         HR|\n",
      "+---+-----+---+-----------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, Inner, NOT (id#107L = id#129L), true\n",
      "   :- Filter isnotnull(id#107L)\n",
      "   :  +- Scan ExistingRDD[id#107L,name#108]\n",
      "   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=750]\n",
      "      +- Filter isnotnull(id#129L)\n",
      "         +- Scan ExistingRDD[id#129L,dept#130]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example in PySpark:\n",
    "\n",
    "small_df = spark.createDataFrame([(1, \"HR\"), (2, \"Engineering\")], [\"id\", \"dept\"])\n",
    "\n",
    "# Non-equi join using Broadcast Nested Loop Join\n",
    "result = df1.join(broadcast(small_df), df1.id != small_df.id, \"inner\")\n",
    "result.show()\n",
    "result.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "648674fd-e9e0-4fc2-a54c-3821c75e0565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85b55c3f-9cc6-4f6c-8372-047337dba595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66eda9fc-924e-44a6-b588-71fe4ffb1096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inteview Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28df4253-2d87-4a5e-bc67-11d88fc22b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why is join an expensive (wide dependency) transformation?\n",
    "\n",
    "Joins in Spark are considered expensive because they involve wide dependencies, meaning that one output partition depends on data from multiple input partitions. This often requires data shuffling, which can significantly impact performance due to the following reasons:\n",
    "\n",
    "**Data Shuffling:**\n",
    "- During a join, Spark often needs to redistribute data across the cluster to ensure that rows with matching keys are colocated on the same partition.\n",
    "- This network-intensive operation involves serializing, transferring, and deserializing large volumes of data.\n",
    "\n",
    "**Sorting:**\n",
    "- In the case of a Sort-Merge Join, data in each partition needs to be sorted by the join key before merging, adding computational overhead.\n",
    "\n",
    "**Memory Overhead:**\n",
    "- For hash-based joins, a hash table is created in memory, which can cause memory pressure if the dataset is large.\n",
    "\n",
    "**Stragglers:**\n",
    "- Imbalanced data distribution can lead to straggler tasks, where some partitions take much longer to process, delaying the entire job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e21700b-adb3-4e9f-ab1b-2f4cb5164951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7185adb5-2218-4a6f-af5c-d515dad8e06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Difference between Shuffle Hash Join and Shuffle Sort-Merge Join\n",
    "\n",
    "| **Aspect**             | **Shuffle Hash Join**                                      | **Shuffle Sort-Merge Join**                                    |\n",
    "|-------------------------|-----------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Dataset Size**        | Suitable for small to medium-sized datasets.              | Suitable for large datasets that don't fit in memory.          |\n",
    "| **Shuffling**           | Requires shuffling both datasets by the join key.         | Requires shuffling both datasets and sorting by the key.       |\n",
    "| **Processing**          | Builds a hash table for one partition and probes it with the other. | Performs a merge operation after sorting the partitions.       |\n",
    "| **Memory Requirement**  | Higher memory usage since it builds hash tables in memory. | Lower memory usage compared to hash join but CPU-intensive due to sorting. |\n",
    "| **Join Type**           | Efficient for equi-joins (exact matches).                 | Optimized for equi-joins but required for sorted data.          |\n",
    "| **Performance**         | Faster for small-medium datasets, but can fail with memory pressure. | More stable and scalable for very large datasets.              |\n",
    "| **Failure Handling**    | Can cause OutOfMemoryError if hash table is too large.     | Handles large datasets more gracefully.                        |\n",
    "| **Configuration**       | Controlled by disabling sort-merge joins (`spark.sql.join.preferSortMergeJoin = false`). | Default join strategy for large datasets in Spark.             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aec14f4f-0374-4443-a4f2-df2bd691097c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13bb971b-ab3c-4209-a87a-7761497aaa7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Why do we need Broadcast Hash Join?\n",
    "\n",
    "Broadcast Hash Join is used to improve the performance of join operations when one of the datasets is small enough to fit in memory. Instead of shuffling the larger dataset, Spark broadcasts (replicates) the smaller dataset to all nodes, enabling each node to perform the join locally.\n",
    "\n",
    "**Key Points:**\n",
    "- Reduces expensive shuffling of the larger dataset.\n",
    "- Ideal for small-large table joins where the smaller table is significantly smaller than the larger table.\n",
    "- It is much faster than other join strategies when applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33d86d8e-dae8-4ade-9603-6e042bb12c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "834dfb92-fe7a-4846-8e62-2462b36ea274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### How does Broadcast Hash Join work?\n",
    "\n",
    "- Spark broadcasts the smaller dataset to all nodes in the cluster.\n",
    "- Each node in the cluster keeps a copy of the smaller dataset in memory.\n",
    "- The join operation is performed locally on each partition of the larger dataset using the broadcasted dataset.\n",
    "- A hash table is built from the broadcasted dataset to efficiently match rows from the larger dataset.\n",
    "\n",
    "**Steps:**\n",
    "- Broadcast the smaller dataset to all worker nodes.\n",
    "- Build a hash table for the smaller dataset.\n",
    "- For each row in the larger dataset, probe the hash table for matching rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e80a1e70-8d64-4da2-a153-d0d53bba5c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c32f568-992a-44b4-99c9-c9cdfec13e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Difference between Broadcast Hash Join and Shuffle Hash Join?\n",
    "\n",
    "| **Feature**          | **Broadcast Hash Join**                       | **Shuffle Hash Join**                  |\n",
    "|-----------------------|-----------------------------------------------|----------------------------------------|\n",
    "| **Dataset Size**      | One dataset must be small enough to fit in memory. | Both datasets can be large.            |\n",
    "| **Shuffling**         | No shuffling of the larger dataset.          | Requires shuffling of both datasets.   |\n",
    "| **Performance**       | Faster for small-large table joins.          | Slower due to the shuffle overhead.    |\n",
    "| **Hash Table**        | Built on the broadcasted dataset.            | Built on the shuffled partitions.      |\n",
    "| **Use Case**          | Small-large joins.                           | Medium to large dataset joins.         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b782bb90-4e6c-42de-9566-ebe05fbaf3ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b71d0cf-d579-4cda-8db8-1f055a4b7475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### How can we change the broadcast size of a table?\n",
    "\n",
    "The broadcast size of a table is controlled by the configuration property:\n",
    "- **`spark.sql.autoBroadcastJoinThreshold`**\n",
    "- Default Value: 10 MB (in Spark).\n",
    "\n",
    "Set the configuration in your Spark session or Spark SQL:\n",
    "- **`spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"50MB\")`**\n",
    "\n",
    "Use Case:\n",
    "- Increase this value if you have larger datasets that can fit in memory and benefit from broadcast join.\n",
    "- Decrease it to avoid broadcasting overly large datasets that may cause memory issues."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "join_strategy_in_spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
