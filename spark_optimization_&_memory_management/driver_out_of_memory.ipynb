{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d34c4ae-8b57-49ff-83be-fed1ba297a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Memory Management\n",
    "Spark memory management is crucial for ensuring that your Spark applications run efficiently. Spark uses a unified memory management model that handles both execution and storage memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b64ed44e-34c5-4f89-9b17-e49dc5379a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b314bb-7afc-4155-84ae-b24b4a03d935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Driver out of memory\n",
    "- The driver is a Java process where the main() method of your Java/Scala/Python program runs. \n",
    "- It manages the SparkContext, responsible for creating DataFrames, Datasets, and RDDs, and executing SQL, transformations, and actions.\n",
    "- The driver is responsible for maintaining metadata, scheduling tasks, and collecting results, so insufficient memory for these operations can cause an OOM error\n",
    "\n",
    "\n",
    "##### Total Driver Memory:\n",
    "The total memory available to the driver is determined by the combination of:\n",
    "\n",
    "**Heap memory** (`spark.driver.memory`): \n",
    "- The driver uses this memory to store metadata, maintain DAGs, manage tasks, and process data collected to the driver (e.g., via collect()).\n",
    "- Default = 1 GB.\n",
    "- how to set: `--conf spark.driver.memory=4g`\n",
    "\n",
    "\n",
    "**Overhead memory** (`spark.driver.memoryOverhead`):\n",
    "- Specifies the amount of non-heap memory allocated to the driver for:\n",
    "  - JVM overhead, such as native threads and direct memory buffers.\n",
    "  - Other Spark internal tasks like shuffling and network communication.\n",
    "- Default = 10% of spark.driver.memory or `384 MB`, whichever is larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22d728c1-4e33-49ac-b9c7-eb74a21444d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "534d2d20-ad11-4d12-bc0d-eb8fe3cb3994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Why do we get Driver OOM?\n",
    "The driver OOM occurs when the Spark driver process runs out of memory. Common driver memory issues include:\n",
    "\n",
    "**1. Collect() Operation:**`\n",
    "- The collect operation in Spark retrieves data from distributed workers and consolidates it on the driver. \n",
    "- This can lead to OOM errors if the collected data is too large to fit into the driver's memory.\n",
    "\n",
    "**2. Broadcast Join:**\n",
    "- Broadcast joins are useful for optimizing joins when one side of the join is small enough to fit in memory. - - - However, if the broadcasted data is too large, it can exhaust driver memory:\n",
    "\n",
    "**Excessive Metadata or:**\n",
    "- The driver stores metadata about RDDs, DataFrames, tasks, and jobs. If the application creates too many objects or stages without proper garbage collection, the metadata can overwhelm the driver's memory.\n",
    "\n",
    "**Improper Memory Allocation:**\n",
    "- Insufficient memory allocation for the driver (`spark.driver.memory` or `spark.driver.memoryOverhead`) can lead to OOM, especially for complex operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "343ce9bf-ec4f-465e-b457-6c3261a88641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a3ebd7e-8f13-48bb-8d36-311dbed1c7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What is driver overhead memory?\n",
    "- Driver Overhead Memory is the additional memory allocated to the driver process for non-heap tasks like:\n",
    "  - JVM overhead\n",
    "  - Garbage collection\n",
    "  - Internal Spark operations (network communication, buffering)\n",
    "- It is controlled by the configuration parameter `spark.driver.memoryOverhead`.\n",
    "- The default value is usually 10% of the total driver memory or 384MB, whichever is greater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10bbd57d-84d5-4d79-ad54-fa2fc6abfbc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a0169bf-6a83-4dc2-97ee-2a03bf9dc8f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### How to Handle Driver OOM?\n",
    "\n",
    "**Avoid Collecting Large Data:**\n",
    "- Minimize the use of `collect()`, `take()`, and `toPandas()`. Process data at the executor level using distributed transformations like `map`, `filter`, or `reduce`.\n",
    "\n",
    "**Broadcast Wisely:**\n",
    "- Use `broadcast()` sparingly and only for small variables.\n",
    "\n",
    "**Increase Driver Memory:**\n",
    "- Allocate more memory to the driver using:\n",
    "  ```bash\n",
    "  --conf spark.driver.memory=4g\n",
    "  --conf spark.driver.memoryOverhead=1g\n",
    "  ```\n",
    "\n",
    "**Cache and Persist Effectively:**\n",
    "- Cache only the required intermediate results to reduce memory usage.\n",
    "\n",
    "**Efficient Query Design:**\n",
    "- Use narrow transformations where possible.\n",
    "- Avoid generating excessive stages and shuffle operations."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "driver_out_of_memory",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
