{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4756b74-6485-4a07-8dc2-eaccfb8b1c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Repartition() vs Coalesce()\n",
    "\n",
    "In Apache Spark, both repartition() and coalesce() are methods used to control the partitioning of data in a `RDD` or a `DataFrame`, but they serve slightly different purposes and have performance implications.\n",
    "\n",
    "Proper partitioning can have a significant impact on the performance and efficiency of your Spark job.\n",
    "\n",
    "### Repartition()\n",
    "- Used for increasing or decreasing the number of partitions.\n",
    "- Always performs a full shuffle across the cluster, which can be an expensive operation.\n",
    "- **Syntax**: `df.repartition(num_partitions)`\n",
    "- **Example**: If you have a DataFrame with 4 partitions and want to increase it to 8, `df.repartition(8)` will redistribute the data across 8 partitions.\n",
    "\n",
    "#### Scenarios:\n",
    "\n",
    "##### 1. Balancing Workload with Skewed Data\n",
    "\n",
    "- *Problem:* Skewed data in join operations causing performance issues.\n",
    "- *Solution:* Use repartition to redistribute data evenly before the join, ensuring a balanced workload.\n",
    "- *Example:* \n",
    "  ```python\n",
    "  skewed_rdd.repartition(10).join(normal_rdd)\n",
    "  ```\n",
    "\n",
    "##### 2. Optimizing Grouping Operations\n",
    "\n",
    "- *Problem:* Uneven data distribution affecting groupByKey or reduceByKey.\n",
    "- *Solution:* Apply repartition before grouping to enhance data distribution.\n",
    "- *Example:*\n",
    "  ```python\n",
    "  original_rdd.repartition(20).groupByKey()\n",
    "  ```\n",
    "\n",
    "\n",
    "### Coalesce()\n",
    "- Used for decreasing the number of partitions.\n",
    "- Avoids a shuffle when reducing partitions, as it tries to reduce partitions by merging data within existing partitions.\n",
    "- Primarily used to decrease the number of partitions when moving data to fewer nodes, often for optimized final stages like saving data to storage. However, it is not efficient for increasing partitions or balancing data across nodes.\n",
    "- **Syntax**: `df.coalesce(num_partitions)`\n",
    "- **Example**: If you have a DataFrame with 8 partitions and want to reduce it to 4, df.coalesce(4) will reduce the partitions without a full shuffle.\n",
    "\n",
    "#### Scenarios: \n",
    "\n",
    "##### 1. Final Stage Data Reduction\n",
    "- *Problem:* High partition count at the end of processing, leading to numerous small output files.\n",
    "- *Solution:* Use coalesce to decrease partitions before saving the final result.\n",
    "- *Example:* \n",
    "  ```python\n",
    "  intermediate_rdd.coalesce(1).saveAsTextFile(\"final_output\")\n",
    "  ```\n",
    "\n",
    "##### 2. Aggregating Small Files\n",
    "- *Problem:* Numerous small files causing storage and reading inefficiencies.\n",
    "- *Solution:* Utilize coalesce to reduce output file count.\n",
    "- Example:\n",
    "  ```python\n",
    "  processed_data.coalesce(5).write.parquet(\"output_data.parquet\")\n",
    "  ```\n",
    "\n",
    "\n",
    "### Choosing between them:\n",
    "- Use `repartition` for significant changes in partition count.\n",
    "- Use `coalesce` for reducing partitions with minimal shuffling, which is more efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0c22c88-cd88-4d84-b410-35dba35d4511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21078af1-7e87-46dd-b915-cb3e67e1a90e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79db7f4a-abdf-4403-8ab2-45dec87e3275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20aa3ffa-4dc7-458a-8437-e02ed8292c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "babd0f18-df08-49b6-921b-74c6a23e40d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_df = spark.read.format(\"csv\") \\\n",
    "          .option(\"header\", \"true\") \\\n",
    "          .option(\"inferSchema\", \"true\") \\\n",
    "          .load(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/2010_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5530bc7-5508-4392-9fe1-3f734a0a1c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n|       United States|             Russia|  156|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "flights_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4ff9d7-5910-4475-a69d-f83a75af7932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 255"
     ]
    }
   ],
   "source": [
    "flights_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab6345a-dc45-467c-9f44-a08bb0c89608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: 1"
     ]
    }
   ],
   "source": [
    "# checking the number of partitions: currently we have only 1 partitions\n",
    "flights_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e09967-9ab1-4ab0-a152-798d922ad754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### partitioning the data using repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404a9039-2c2b-4f0a-943e-b6095673b29c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partition_flights_df = flights_df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efdefb1b-eedc-427e-8674-ce4a03b54268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: 4"
     ]
    }
   ],
   "source": [
    "# now you can see the we have 4 partitions\n",
    "partition_flights_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89aefeb1-7344-41d9-a987-9a312b8d28bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|partitionID|count|\n+-----------+-----+\n|          0|   63|\n|          1|   64|\n|          2|   64|\n|          3|   64|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# now lets check in each partition how much data we have\n",
    "partition_flights_df.withColumn(\"partitionID\", spark_partition_id()).groupBy(\"partitionID\").count().show()\n",
    "\n",
    "\n",
    "# as you can see repartition() distributed the data evenly across all the partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d50d4642-43e4-4922-bd95-7b0bf3cfc2df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### spark_partition_id()\n",
    "- In Spark, spark_partition_id() is a function that returns the partition ID of each row in a DataFrame or RDD. \n",
    "- This function is often used for understanding the distribution of data across partitions, which can be helpful for debugging and optimizing Spark jobs.\n",
    "- The spark_partition_id() function is available in PySpark through pyspark.sql.functions. You can use it to inspect how rows are distributed across partitions by adding it as a column in your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6be7d2-88c5-463d-b449-22e7c76a45ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we can also do repartition on any specific columns as well\n",
    "partition_on_column_df = flights_df.repartition(300, \"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621efd15-afa7-4bad-92c8-4c5835a3781f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: 300"
     ]
    }
   ],
   "source": [
    "partition_on_column_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0b9717d-72dc-472f-9d08-0068220c7403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|partitionID|count|\n+-----------+-----+\n|          0|    1|\n|          2|    2|\n|          7|    1|\n|         10|    1|\n|         13|    1|\n|         15|    2|\n|         16|    2|\n|         21|    1|\n|         22|    1|\n|         28|    1|\n|         31|    1|\n|         39|    1|\n|         42|    1|\n|         43|    1|\n|         44|    1|\n|         45|    2|\n|         48|    1|\n|         53|    1|\n|         54|    1|\n|         55|    1|\n+-----------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# now lets check in each partition how much data we have\n",
    "partition_on_column_df.withColumn(\"partitionID\", spark_partition_id()).groupBy(\"partitionID\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c0ca764-5ab7-44b0-973d-783b498deeb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### NOTE:\n",
    "- in the above example, we partition teh data into 300 paritions on column but we have only 255 records in our df\n",
    "- in this case the spark wil assing some null values to the few partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "670095b0-9516-42c7-b86f-337194fea33b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09f2f1c5-d621-4411-80fe-3ddd9bc22cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### partitioning the data using coaleasce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fdba9ae-47d5-4da7-95a7-9530225a8bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|partitionID|count|\n+-----------+-----+\n|          0|   32|\n|          1|   31|\n|          2|   32|\n|          3|   32|\n|          4|   32|\n|          5|   32|\n|          6|   32|\n|          7|   32|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# first lets create 8 partitions\n",
    "\n",
    "partition_flights_df = flights_df.repartition(8)\n",
    "partition_flights_df.withColumn(\"partitionID\", spark_partition_id()).groupBy(\"partitionID\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c493b004-5bb1-4d06-a5c6-d62e9a7a7509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now lets create 3 partitions using coalesce\n",
    "three_coaleasce_df = partition_flights_df.coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a324b9b-513e-48cb-a840-025f29e6d13a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|partitionID|count|\n+-----------+-----+\n|          0|   63|\n|          1|   96|\n|          2|   96|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# now let see how coaleasce distributed the data \n",
    "three_coaleasce_df.withColumn(\"partitionID\", spark_partition_id()).groupBy(\"partitionID\").count().show()\n",
    "\n",
    "# as you can see, it just merged the data into 3 papritions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e311b72-0057-4154-8bea-4a3ed50d142a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|partitionID|count|\n+-----------+-----+\n|          0|   85|\n|          1|   85|\n|          2|   85|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# if we repartition to 3 we will get even partitions data\n",
    "repartition_df = partition_flights_df.repartition(3)\n",
    "repartition_df.withColumn(\"partitionID\", spark_partition_id()).groupBy(\"partitionID\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8486d1cf-a852-4cc0-b36e-6badf5cb9ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Spark Repartition() vs Coalesce()",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
