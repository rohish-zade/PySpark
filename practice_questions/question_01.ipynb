{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c24396-f65e-46b8-a4b3-50fcbb1d136f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ùêêùêÆùêûùê¨ùê≠ùê¢ùê®ùêß 1: Find the top N most frequent words in a large text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a278f13-07cd-4c80-8349-d4245b250b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Sample Data\n",
    "Let‚Äôs assume you have a text file named sample_1.txt with the following content:\n",
    "\n",
    "```text\n",
    "Hello world\n",
    "Hello from PySpark\n",
    "PySpark is awesome\n",
    "Hello PySpark world\n",
    "```\n",
    "\n",
    "- **Load the Data:** Read the text file into a DataFrame or\n",
    "RDD.\n",
    "- **Tokenize the Text:** Split the text into words.\n",
    "- **Count Word Frequencies:** Count the occurrences of\n",
    "each word.\n",
    "- **Sort and Extract Top N Words:** Sort the words by\n",
    "frequency and extract the top N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2adfdd48-7741-47c5-bce0-b49e71a9617a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|              value|\n+-------------------+\n|        Hello world|\n| Hello from PySpark|\n| PySpark is awesome|\n|Hello PySpark world|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, explode, lower\n",
    "\n",
    "# load the data\n",
    "file_path = \"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/sample_1.text\"\n",
    "df = spark.read.text(file_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb99295-634a-45d8-83ee-94e06d7507bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n|word                   |\n+-----------------------+\n|[Hello, world]         |\n|[Hello, from, PySpark] |\n|[PySpark, is, awesome] |\n|[Hello, PySpark, world]|\n+-----------------------+\n\n+-------+\n|word   |\n+-------+\n|Hello  |\n|world  |\n|Hello  |\n|from   |\n|PySpark|\n|PySpark|\n|is     |\n|awesome|\n|Hello  |\n|PySpark|\n|world  |\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# tokenized the text into words\n",
    "words_df = df.select(split(col(\"Value\"), \" \").alias(\"word\"))\n",
    "words_df.show(truncate=False)\n",
    "\n",
    "words_df = df.select(explode(split(col(\"Value\"), \" \")).alias(\"word\"))\n",
    "words_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e7a7c25-e394-4aca-8020-ce9f34b20796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|   word|count|\n+-------+-----+\n|  hello|    3|\n|     is|    1|\n|pyspark|    3|\n|   from|    1|\n|  world|    2|\n|awesome|    1|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Convert to lower case for case insensitivity\n",
    "words_df = words_df.withColumn(\"word\", lower(col(\"word\")))\n",
    "\n",
    "# Count the occurrences of each word.\n",
    "word_counts_df = words_df.groupBy(col(\"word\")).count()\n",
    "word_counts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed669f18-0d8e-411c-95fa-131640596c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|   word|count|\n+-------+-----+\n|  hello|    3|\n|pyspark|    3|\n|  world|    2|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort the words by frequency and extract the top N. let say N=3\n",
    "top_n = 3\n",
    "sorted_word_count_df = word_counts_df.sort(col(\"count\").desc())\n",
    "top_words_df = sorted_word_count_df.limit(top_n)\n",
    "\n",
    "# show the results\n",
    "top_words_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f90e8d-c5e8-4244-a818-830706a6668b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **explode():** is used to transform an array or map column into multiple rows. It helps in flattening nested data structures, particularly when dealing with arrays."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1290180560662839,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "question_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
