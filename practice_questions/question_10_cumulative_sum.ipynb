{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eb7aa4-7f33-454a-9733-d9d6969e7c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Running total of stock prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78be65e5-f6e5-4b13-acf0-e33fcc0b52ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are given a dataset containing daily stock prices. Write a PySpark program to calculate the running total of stock prices for each stock symbol in the dataset.\n",
    "\n",
    "\n",
    "data = [ (\"2024-09-01\", \"AAPL\", 150), (\"2024-09-02\", \"AAPL\", 160), \n",
    "(\"2024-09-03\", \"AAPL\", 170), (\"2024-09-01\", \"GOOGL\", 1200),\n",
    " (\"2024-09-02\", \"GOOGL\", 1250), (\"2024-09-03\", \"GOOGL\", 1300) ] \n",
    "\n",
    "\n",
    "**output:**\n",
    "| date       | symbol | price | cumulative_price |\n",
    "|------------|--------|-------|------------------|\n",
    "| 2024-09-01 | AAPL   | 150   | 150              |\n",
    "| 2024-09-02 | AAPL   | 160   | 310              |\n",
    "| 2024-09-03 | AAPL   | 170   | 480              |\n",
    "| 2024-09-01 | GOOGL  | 1200  | 1200             |\n",
    "| 2024-09-02 | GOOGL  | 1250  | 2450             |\n",
    "| 2024-09-03 | GOOGL  | 1300  | 3750             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da53cac1-1669-4d2a-a9c9-c977e3bb8e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n|      date|symbol|price|\n+----------+------+-----+\n|2024-09-01|  AAPL|  150|\n|2024-09-02|  AAPL|  160|\n|2024-09-03|  AAPL|  170|\n|2024-09-01| GOOGL| 1200|\n|2024-09-02| GOOGL| 1250|\n|2024-09-03| GOOGL| 1300|\n+----------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "data = [ (\"2024-09-01\", \"AAPL\", 150), (\"2024-09-02\", \"AAPL\", 160), (\"2024-09-03\", \"AAPL\", 170), (\"2024-09-01\", \"GOOGL\", 1200), (\"2024-09-02\", \"GOOGL\", 1250), (\"2024-09-03\", \"GOOGL\", 1300) ] \n",
    "\n",
    "columns = [\"date\", \"symbol\", \"price\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f8023f-5df1-46c9-ba51-c0493bb64b6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----------------+\n|      date|symbol|price|cumulative_price|\n+----------+------+-----+----------------+\n|2024-09-01|  AAPL|  150|             150|\n|2024-09-02|  AAPL|  160|             310|\n|2024-09-03|  AAPL|  170|             480|\n|2024-09-01| GOOGL| 1200|            1200|\n|2024-09-02| GOOGL| 1250|            2450|\n|2024-09-03| GOOGL| 1300|            3750|\n+----------+------+-----+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum \n",
    "\n",
    "# The built-in Python sum() is not designed for PySpark columns. Instead, use pyspark.sql.functions.sum(), which correctly handles column operations in a distributed manner\n",
    "\n",
    "# default window frame is rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "window = Window.partitionBy(col(\"symbol\")).orderBy(col(\"price\"))\n",
    "\n",
    "stock_df = df.withColumn(\"cumulative_price\", sum(col(\"price\")).over(window))\n",
    "stock_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "question_10_cumulative_sum",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}