{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b032798-86cf-4142-99bb-73b50769518a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handle time-series data in PySpark\n",
    "\n",
    "Time Series Data: Data that is indexed or organized by time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f78bcfb9-4297-40bc-93c4-1be4637a95c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Common Operations:\n",
    "- **Date Parsing:** Convert string data into date or timestamp format.\n",
    "- **Resampling:** Aggregate data based on a specific time interval (hourly, daily, monthly).\n",
    "- **Time-based Filtering:** Filter data by a date or time range.\n",
    "- **Lagging/Leading:** Comparing values from previous or subsequent time periods.\n",
    "- **Rolling/Aggregating:** Calculating statistics like moving averages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac92fd8d-f39e-46da-ba07-c28b60fb65b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d38743f9-0c9e-47a1-a408-a86bb28cdbd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|          timestamp|value|\n+-------------------+-----+\n|2024-01-01 08:00:00| 10.5|\n|2024-01-01 09:00:00| 12.3|\n|2024-01-01 10:00:00| 15.6|\n|2024-01-01 11:00:00| 11.4|\n|2024-01-01 12:00:00| 14.7|\n|2024-01-01 13:00:00| 13.5|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample time-series data\n",
    "data = [\n",
    "    ('2024-01-01 08:00:00', 10.5),\n",
    "    ('2024-01-01 09:00:00', 12.3),\n",
    "    ('2024-01-01 10:00:00', 15.6),\n",
    "    ('2024-01-01 11:00:00', 11.4),\n",
    "    ('2024-01-01 12:00:00', 14.7),\n",
    "    ('2024-01-01 13:00:00', 13.5),\n",
    "]\n",
    "\n",
    "# Columns\n",
    "columns = ['timestamp', 'value']\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b2c797-5aa0-4f53-bd3c-5d011715d63f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|          timestamp|value|\n+-------------------+-----+\n|2024-01-01 08:00:00| 10.5|\n|2024-01-01 09:00:00| 12.3|\n|2024-01-01 10:00:00| 15.6|\n|2024-01-01 11:00:00| 11.4|\n|2024-01-01 12:00:00| 14.7|\n|2024-01-01 13:00:00| 13.5|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# converting 'timepstamp' column to a timepstamp type\n",
    "df = df.withColumn(\"timestamp\", F.col(\"timestamp\").cast(\"timestamp\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "325c8575-3ee7-46b1-adc1-c8123c5d25f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50d8f23c-a1f7-4452-b10f-2652c3c72f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Common Time-Series Operations in PySpark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d99f7b8-c5ce-477e-90cc-957dc3038ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07b3c3a7-624e-4f97-829b-951e96f4f420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1.Resampling (Aggregation by Time Intervals):**\n",
    "\n",
    "Suppose you want to aggregate data on an hourly basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b447c6-56e1-4dcf-9f41-06776506bd29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------+\n|window                                    |avg_value|\n+------------------------------------------+---------+\n|{2024-01-01 08:00:00, 2024-01-01 09:00:00}|10.5     |\n|{2024-01-01 09:00:00, 2024-01-01 10:00:00}|12.3     |\n|{2024-01-01 10:00:00, 2024-01-01 11:00:00}|15.6     |\n|{2024-01-01 11:00:00, 2024-01-01 12:00:00}|11.4     |\n|{2024-01-01 12:00:00, 2024-01-01 13:00:00}|14.7     |\n|{2024-01-01 13:00:00, 2024-01-01 14:00:00}|13.5     |\n+------------------------------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# group by an hour and calculate  the average value\n",
    "hourly_avg = df.groupBy(F.window(\"timestamp\", \"1 hour\")).agg(F.avg(\"value\").alias(\"avg_value\"))\n",
    "\n",
    "hourly_avg.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80297595-dbc8-44e7-ab40-ad06f7336b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2.Filtering by Date Range:**\n",
    "\n",
    "To filter data between two specific dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8d6315-5617-4a0c-8a38-ad822c910553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|          timestamp|value|\n+-------------------+-----+\n|2024-01-01 09:00:00| 12.3|\n|2024-01-01 10:00:00| 15.6|\n|2024-01-01 11:00:00| 11.4|\n|2024-01-01 12:00:00| 14.7|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "start_date = '2024-01-01 09:00:00'\n",
    "end_date = '2024-01-01 12:00:00'\n",
    "\n",
    "filtered_df = df.filter((F.col(\"timestamp\") >= start_date) & (F.col(\"timestamp\") <= end_date))\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20cba7f4-a55d-4f3f-a04d-730edb2a5e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3.Lag and Lead Operations:**\n",
    "\n",
    "To calculate the difference between current and previous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e85fd91-d2cc-4eb3-a19c-3b24d098000d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+----------+\n|          timestamp|value|prev_value|\n+-------------------+-----+----------+\n|2024-01-01 08:00:00| 10.5|      null|\n|2024-01-01 09:00:00| 12.3|      10.5|\n|2024-01-01 10:00:00| 15.6|      12.3|\n|2024-01-01 11:00:00| 11.4|      15.6|\n|2024-01-01 12:00:00| 14.7|      11.4|\n|2024-01-01 13:00:00| 13.5|      14.7|\n+-------------------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_specs = Window.orderBy(\"timestamp\")\n",
    "\n",
    "df_with_lag = df.withColumn(\"prev_value\", F.lag(\"value\", 1).over(window_specs))\n",
    "df_with_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f1e95c-ecf1-4b3d-a508-dbfda5322d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+----------+----------+\n|          timestamp|value|prev_value|value_diff|\n+-------------------+-----+----------+----------+\n|2024-01-01 08:00:00| 10.5|      null|      null|\n|2024-01-01 09:00:00| 12.3|      10.5|       1.8|\n|2024-01-01 10:00:00| 15.6|      12.3|       3.3|\n|2024-01-01 11:00:00| 11.4|      15.6|      -4.2|\n|2024-01-01 12:00:00| 14.7|      11.4|       3.3|\n|2024-01-01 13:00:00| 13.5|      14.7|      -1.2|\n+-------------------+-----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_lag = df_with_lag.withColumn(\"value_diff\", F.round(F.col(\"value\") - F.col(\"prev_value\"), 2))\n",
    "df_with_lag.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ccbdb20-e4e5-4427-93c2-a3e2e2b38087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4.Rolling Average:**\n",
    "\n",
    "To compute a moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57030815-a844-435a-a677-765609e92e61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------------+\n|          timestamp|value|       rolling_avg|\n+-------------------+-----+------------------+\n|2024-01-01 08:00:00| 10.5|              10.5|\n|2024-01-01 09:00:00| 12.3|              11.4|\n|2024-01-01 10:00:00| 15.6|12.799999999999999|\n|2024-01-01 11:00:00| 11.4|              13.1|\n|2024-01-01 12:00:00| 14.7|              13.9|\n|2024-01-01 13:00:00| 13.5|13.200000000000001|\n+-------------------+-----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "window_specs = Window.orderBy(\"timestamp\").rowsBetween(-2, 0)\n",
    "\n",
    "rolling_avg_df = df.withColumn(\"rolling_avg\", F.avg(\"value\").over(window_specs))\n",
    "rolling_avg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370a2f6c-1561-4a11-8ea7-1c81635561d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- `.rowsBetween(-2, 0): `This is the key part for calculating the rolling average. It defines the range of rows to include in the calculation for each row.\n",
    "- `-2:` This means \"include the row that is two rows before the current row\".\n",
    "- `0:` This means \"include the current row\".\n",
    "- Therefore, for each row, the window will include the current row and the two rows immediately preceding it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "142a6633-344d-45d2-86ae-3f41bf1578f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Detailed Explanation of the rolling_avg Calculation:**\n",
    "\n",
    "- Row 1 (08:00:00): Since there are no preceding rows, the rolling average is simply the value of the first row (10.5).\n",
    "- Row 2 (09:00:00): The rolling average is the average of the first two rows: (10.5 + 12.3) / 2 = 11.4.\n",
    "- Row 3 (10:00:00): The rolling average is the average of the first three rows: (10.5 + 12.3 + 15.6) / 3 = 12.8.\n",
    "- Row 4 (11:00:00): The rolling average is the average of rows 2, 3, and 4: (12.3 + 15.6 + 11.4) / 3 = 13.1.\n",
    "- Row 5 (12:00:00): The rolling average is the average of rows 3, 4, and 5: (15.6 + 11.4 + 14.7) / 3 = 13.9.\n",
    "- Row 6 (13:00:00): The rolling average is the average of rows 4, 5, and 6: (11.4 + 14.7 + 13.5) / 3 = 13.2."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "question_26_handle_time_series_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}