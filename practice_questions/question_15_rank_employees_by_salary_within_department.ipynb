{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f558dc87-39f8-4046-baa9-b69da8b13e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Rank() in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42fa899e-aeda-41c2-b557-bcfe0b250b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How would you use the `rank()` function in PySpark to rank employees based on their salary within their department?\n",
    "\n",
    "=> To rank employees based on their salary within their department using the rank() function in PySpark, you would typically use the Window function to partition the data by the department and order the employees based on their salary in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db02450-a638-4a7d-bfab-90b1a86b76b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample data\n",
    "data = [\n",
    "    (1, 'Alice', 'HR', 5000),\n",
    "    (2, 'Bob', 'HR', 6000),\n",
    "    (3, 'Charlie', 'IT', 7000),\n",
    "    (4, 'David', 'IT', 9000),\n",
    "    (5, 'Eve', 'HR', 5500),\n",
    "    (6, 'Frank', 'IT', 8000)\n",
    "]\n",
    "\n",
    "columns = [\"EmployeeID\", \"Name\", \"Department\", \"Salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a1727f-cd49-4e9e-817c-1095ec65821e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1985111441010120#setting/sparkui/0331-101928-o5qw6fev/driver-9190777006598020145\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1985111441010120#setting/sparkui/0331-101928-o5qw6fev/driver-9190777006598020145\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Rank\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17290f95-ce3a-4bad-829d-d896352c2a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+------+\n|EmployeeID|   Name|Department|Salary|\n+----------+-------+----------+------+\n|         1|  Alice|        HR|  5000|\n|         2|    Bob|        HR|  6000|\n|         3|Charlie|        IT|  7000|\n|         4|  David|        IT|  9000|\n|         5|    Eve|        HR|  5500|\n|         6|  Frank|        IT|  8000|\n+----------+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFram\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1329c08c-a0db-4266-8147-1477a1d23121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+------+----+\n|EmployeeID|   Name|Department|Salary|rank|\n+----------+-------+----------+------+----+\n|         2|    Bob|        HR|  6000|   1|\n|         5|    Eve|        HR|  5500|   2|\n|         1|  Alice|        HR|  5000|   3|\n|         4|  David|        IT|  9000|   1|\n|         6|  Frank|        IT|  8000|   2|\n|         3|Charlie|        IT|  7000|   3|\n+----------+-------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# defind a window specs\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank, desc\n",
    "\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(desc(\"Salary\"))\n",
    "\n",
    "rank_df = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "rank_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "question_15_rank_employees_by_salary_within_department",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}