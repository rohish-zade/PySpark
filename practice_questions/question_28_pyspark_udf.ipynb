{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c22cc328-9d18-4128-9c01-ff39000d37dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explain PySpark UDF with the help of an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8c6314b-3948-4cd1-a7b4-cbb48b04d3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- PySpark **UDF (User-Defined Function)** allows you to create custom functions to perform transformations on data in a DataFrame.\n",
    "- You define a function in Python and then register it as a UDF so that it can be used with PySpark.\n",
    "- These are particularly useful when you need to perform complex transformations or calculations that aren't readily available in PySpark's built-in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "829dc1e6-68d3-463f-83cf-6627b11a8ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Points about UDFs:\n",
    "- You can use a `UDF` to apply any custom logic to a column in a PySpark DataFrame.\n",
    "- The function needs to be registered as a UDF before it can be used.\n",
    "- `UDFs` can be slow because they don't benefit from PySpark's internal optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "944eb13d-7f3c-4cdb-9379-97fc959979c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Steps to create and use a UDF:\n",
    "1. Define a Python function.\n",
    "2. Convert the function into a UDF using `pyspark.sql.functions.udf.`\n",
    "3. Apply the UDF to a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fe5f4ac-6596-472a-a2aa-ca11149a23a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Example:**\n",
    "\n",
    "Letâ€™s assume we have a DataFrame with a column called \"name\" and we want to create a UDF to convert the names to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4980208b-fd5e-4194-b5d8-60a20bc4a6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n|  name| id|\n+------+---+\n|Rohish|  1|\n|Rajesh|  2|\n|Chetan|  3|\n+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "data = [\n",
    "    (\"Rohish\", 1),\n",
    "    (\"Rajesh\", 2),\n",
    "    (\"Chetan\", 3)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"id\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f585020-f048-4915-9aa9-bf7d86a94008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Define a Python function\n",
    "def convert_to_upper(name):\n",
    "    return name.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9eb159-2f74-4745-a7d5-7df7c7f54bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Register the function as a UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Register the udf: Here, udf() wraps your Python function and tells Spark that it returns a string (StringType()).\n",
    "convert_upper_udf = udf(convert_to_upper, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf6a3d3-387d-4f33-9d20-400896b63d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n|  name| id|\n+------+---+\n|ROHISH|  1|\n|RAJESH|  2|\n|CHETAN|  3|\n+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Apply the UDF to the DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "upper_df = df.withColumn(\"name\", convert_upper_udf(col(\"name\")))\n",
    "upper_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e40326df-e649-42ed-ac7f-d04a111522f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3659c6e-5469-4a9e-a831-aaa92d3b4ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4d9bbc-fc62-4887-bc89-3e856c0c1f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# with PySpark in build function\n",
    "from pyspark.sql.functions import col, upper\n",
    "\n",
    "df.withColumn(\"name\", upper(col(\"name\"))).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "question_28_pyspark_udf",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}