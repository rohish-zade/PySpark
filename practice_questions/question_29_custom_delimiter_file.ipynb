{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6375863-5a82-498f-b9be-c989d10b64af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The given file has a delimiter `~|` How will you load it as a spark DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae18dd13-9d49-4239-9130-f5a4dfd159f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sample Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38a6cbdf-027e-4549-b52d-c1514a558724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">id~|name~|age~|city\r\n",
       "1~|Alice~|30~|New York\r\n",
       "2~|Bob~|25~|Los Angeles\r\n",
       "3~|Charlie~|35~|Chicago\r\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">id~|name~|age~|city\r\n1~|Alice~|30~|New York\r\n2~|Bob~|25~|Los Angeles\r\n3~|Charlie~|35~|Chicago\r\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "head dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/delimited.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f12b987-94d7-47ef-a10d-bead8c01a597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Steps to Load the File as a DataFrame:**\n",
    "1. Set up Spark Session: You need a Spark session to interact with Spark.\n",
    "2. Read the file with the custom delimiter: Use the csv method of spark.read and specify the `~|` delimiter using\n",
    "option(\"delimiter\",\"~|\").\n",
    "3. Infer or specify schema: Optionally, you can infer the schema or define it manually.\n",
    "4. Show the DataFrame: Once the data is loaded, you can display the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33614e6-b7cd-4831-9ca7-6db0af764c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+\n| id|   name|age|       city|\n+---+-------+---+-----------+\n|  1|  Alice| 30|   New York|\n|  2|    Bob| 25|Los Angeles|\n|  3|Charlie| 35|    Chicago|\n+---+-------+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Load File with Custom Delimiter\").getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"delimiter\", \"~|\") \\\n",
    "            .load(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/delimited.csv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f292f5bc-8212-4481-a142-2dbe49623edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "- `option(\"header\",\"true\"):` Specifies that the first line of the file contains column names.\n",
    "- `option(\"delimiter\",\"~|\"):` Sets the custom delimiter `~|` to correctly parse the file.\n",
    "- `.csv():` Loads the file as a CSV since it is in text format, but uses the specified delimiter."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3604164615903443,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "question_29_custom_delimiter_file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}