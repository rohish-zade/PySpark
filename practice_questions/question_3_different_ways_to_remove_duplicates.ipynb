{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa83758-ec6b-4827-a540-ad12935f190f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Different ways to remove duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3bc0f6-063d-41dc-b690-0ba66217c9c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Write a PySpark code to remove duplicate rows based on specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e2be0a-d1b8-46b9-a109-84100b122d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| ID|  Name|Salary|\n+---+------+------+\n|  1|Rohish|  5000|\n|  2|Melody|  3000|\n|  3|Rohish|  2000|\n|  4|Rajesh|  4000|\n|  1|Rohish|  5000|\n|  6|Melody|  3000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "data = [ (1, \"Rohish\", 5000), (2, \"Melody\", 3000), \n",
    "         (3, \"Rohish\", 2000), (4, \"Rajesh\", 4000), \n",
    "         (1, \"Rohish\", 5000), (6, \"Melody\", 3000) \n",
    "       ]\n",
    "\n",
    "columns = [\"ID\", \"Name\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22074498-9d8f-40cf-9c71-c4b8abebdded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### In PySpark, you can remove duplicates using multiple methods depending on your requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018df010-73ab-4a4d-8bc8-8c4d788b27a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Using dropDuplicates():** This is the simplest way to remove duplicates based on specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c1154d-47a2-4c15-b2a0-b16ad0a779da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dropping duplicates:\n+---+------+------+\n| ID|  Name|Salary|\n+---+------+------+\n|  1|Rohish|  5000|\n|  2|Melody|  3000|\n|  3|Rohish|  2000|\n|  4|Rajesh|  4000|\n|  1|Rohish|  5000|\n|  6|Melody|  3000|\n+---+------+------+\n\nafter dropping duplicates:\n+---+------+------+\n| ID|  Name|Salary|\n+---+------+------+\n|  1|Rohish|  5000|\n|  2|Melody|  3000|\n|  4|Rajesh|  4000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"before dropping duplicates:\")\n",
    "df.show()\n",
    "\n",
    "# column names should be a list or tuple\n",
    "df_unique = df.dropDuplicates([\"Name\"])\n",
    "\n",
    "print(\"after dropping duplicates:\")\n",
    "df_unique.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c80d336-2b70-47a1-a5ae-4280918b012e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| ID|  Name|Salary|\n+---+------+------+\n|  1|Rohish|  5000|\n|  2|Melody|  3000|\n|  3|Rohish|  2000|\n|  4|Rajesh|  4000|\n|  6|Melody|  3000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# If you want to drop duplicates based on all columns:\n",
    "df_unique = df.dropDuplicates()\n",
    "df_unique.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9a5f892-7428-409b-8b3e-f0033ddf010e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "882c4f1b-586e-4537-affc-14420d195178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Using distinct():** This removes duplicate rows from the entire DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63720df5-7f48-4a6c-a7bf-6f064d265f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| ID|  Name|Salary|\n+---+------+------+\n|  1|Rohish|  5000|\n|  2|Melody|  3000|\n|  3|Rohish|  2000|\n|  4|Rajesh|  4000|\n|  6|Melody|  3000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_distinct = df.distinct()\n",
    "df_distinct.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "637e388b-fbe7-4f8e-bf27-e13913861944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**`distinct()` considers all columns while `dropDuplicates([\"col1\", \"col2\"])` removes duplicates based on selected columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c057c6d4-0f7e-4d68-97a6-9e07b33a40aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4b90bab-2ec2-4ec2-93df-5c629f540669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Using Window Functions (row_number())**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996c436f-9b4c-462d-9cce-d713c374eb57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+\n| ID|  Name|Salary|rank|\n+---+------+------+----+\n|  2|Melody|  3000|   1|\n|  6|Melody|  3000|   2|\n|  4|Rajesh|  4000|   1|\n|  1|Rohish|  5000|   1|\n|  3|Rohish|  2000|   2|\n|  1|Rohish|  5000|   3|\n+---+------+------+----+\n\n+---+------+------+\n| ID|  Name|Salary|\n+---+------+------+\n|  2|Melody|  3000|\n|  4|Rajesh|  4000|\n|  1|Rohish|  5000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "window = Window.partitionBy(\"Name\").orderBy(\"Name\")\n",
    "rank_df = df.withColumn(\"rank\", row_number().over(window))\n",
    "rank_df.show()\n",
    "\n",
    "rank_df = rank_df.filter(col(\"rank\")==1).drop(\"rank\")\n",
    "rank_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0289dae-691a-46d4-ab74-8c7eb83da46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e6c0b9-f610-4301-8672-0a37b9ac4072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Using `groupBy()` + `agg()`:** To check how many duplicates exist before removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7fb105-6c59-4fb9-902f-53b7dd49212c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|  Name|count|\n+------+-----+\n|Rohish|    3|\n|Melody|    2|\n+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "df_grouped = df.groupBy(\"Name\").agg(count(\"*\").alias(\"count\"))\n",
    "df_grouped.filter(col(\"count\") > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61ce9025-28ed-4c05-842c-7ae836eb1427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b694f3d4-bbfb-42c5-8dd2-64578f67ab3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Which Method to Use?\n",
    "\n",
    "| **Method**        | **Use Case** |\n",
    "|------------------|-----------------------------------------------|\n",
    "| `dropDuplicates()` | Best for quick duplicate removal on specific columns |\n",
    "| `distinct()`      | Best when you want unique rows based on all columns |\n",
    "| `row_number()`    | Best when you need to retain a record based on ordering |\n",
    "| `groupBy()`       | Best when you need to aggregate duplicate records |\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "question_3_different_ways_to_remove_duplicates",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
