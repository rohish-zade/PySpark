{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7151bf-8f13-41b6-9b96-ebaada1765b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Different Ways to Read Data into PySpark\n",
    "\n",
    "In PySpark, there are various ways to read data from different sources such as CSV, JSON, Parquet, ORC, and databases like MySQL. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190bad73-6326-4bad-b5c4-93c46ea73173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c2c4f6-7152-40c9-aac8-b8c37e47b9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Reading CSV File:**\n",
    "\n",
    "PySpark provides read.csv() to load data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2232c610-7352-4486-86c0-b0fc01f934bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------+------+------------+---------+\n",
      "|employee_id|        name|department|salary|joining_date| location|\n",
      "+-----------+------------+----------+------+------------+---------+\n",
      "|        101|Rohan Sharma|        IT| 75000|  2020-05-12|Bangalore|\n",
      "|        102|  Priya Iyer|        HR| 65000|  2019-08-25|    Delhi|\n",
      "|        103|Rajesh Kumar|   Finance| 80000|  2021-03-15|   Mumbai|\n",
      "|        104| Sneha Patil|        IT| 78000|  2018-07-30|     Pune|\n",
      "|        105| Amit Sharma| Marketing| 72000|  2022-01-10|Hyderabad|\n",
      "|        106|  Ananya Das|        HR| 67000|  2017-11-20|  Kolkata|\n",
      "|        107|Vikram Singh|   Finance| 85000|  2023-06-05|  Chennai|\n",
      "|        108| Rohit Verma|        IT| 76000|  2020-09-18|Bangalore|\n",
      "|        109| Arjun Mehta| Marketing| 73000|  2019-12-11|    Delhi|\n",
      "|        110| Rohish Zade|   Finance| 81000|  2016-04-22|   Mumbai|\n",
      "+-----------+------------+----------+------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df = spark.read.csv(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/employee_data.csv\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d88bbcc-c741-4a08-8521-ed2965e43dda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# csv_df.write.format(\"parquet\").save(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/parquet_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb596b52-5c94-4296-b4ea-910714649adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Reading JSON File:**\n",
    "\n",
    "To read data from a JSON file, use read.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "013ba924-d24c-4339-bd0b-181cf310f03c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|age|    name|salary|\n",
      "+---+--------+------+\n",
      "| 20|  Manish| 20000|\n",
      "| 25|  Nikita| 21000|\n",
      "| 16|  Pritam| 22000|\n",
      "| 35|Prantosh| 25000|\n",
      "| 67|  Vikash| 40000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df = spark.read.json(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/line_delimited_json.json\")\n",
    "json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da3db5b-1525-4e33-9319-e6d3f65bac3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# json_df.write.format(\"orc\").save(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/orc_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e9f43de-0330-42cd-816f-915c0682ae9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Reading Parquet File:**\n",
    "\n",
    "Parquet is a columnar file format, and PySpark provides read.parquet() to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a3db16-54d2-4713-bfab-030a068dd5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------+------+------------+---------+\n",
      "|employee_id|        name|department|salary|joining_date| location|\n",
      "+-----------+------------+----------+------+------------+---------+\n",
      "|        101|Rohan Sharma|        IT| 75000|  2020-05-12|Bangalore|\n",
      "|        102|  Priya Iyer|        HR| 65000|  2019-08-25|    Delhi|\n",
      "|        103|Rajesh Kumar|   Finance| 80000|  2021-03-15|   Mumbai|\n",
      "|        104| Sneha Patil|        IT| 78000|  2018-07-30|     Pune|\n",
      "|        105| Amit Sharma| Marketing| 72000|  2022-01-10|Hyderabad|\n",
      "|        106|  Ananya Das|        HR| 67000|  2017-11-20|  Kolkata|\n",
      "|        107|Vikram Singh|   Finance| 85000|  2023-06-05|  Chennai|\n",
      "|        108| Rohit Verma|        IT| 76000|  2020-09-18|Bangalore|\n",
      "|        109| Arjun Mehta| Marketing| 73000|  2019-12-11|    Delhi|\n",
      "|        110| Rohish Zade|   Finance| 81000|  2016-04-22|   Mumbai|\n",
      "+-----------+------------+----------+------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_df = spark.read.parquet(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/data.parquet\")\n",
    "parquet_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "857e0994-8561-4ebc-adc0-12ad22d3ce8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. Reading ORC File:**\n",
    "\n",
    "ORC (Optimized Row Columnar) files are commonly used in Hive for efficient storage and processing of large datasets in Hadoop.\n",
    "You can read them using read.orc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0059b91c-d57e-4a03-bcd9-b218ec50443d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|age|    name|salary|\n",
      "+---+--------+------+\n",
      "| 20|  Manish| 20000|\n",
      "| 25|  Nikita| 21000|\n",
      "| 16|  Pritam| 22000|\n",
      "| 35|Prantosh| 25000|\n",
      "| 67|  Vikash| 40000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orc_df = spark.read.orc(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/orc_data.orc\")\n",
    "orc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11f06132-db32-4d0d-bd27-0415e9d3e405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Reading Data from SQL Server (JDBC):**\n",
    "\n",
    "You can also read data from relational databases like MySQLusing the JDBC connector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53ba1906-f516-4443-be13-414bdc15c491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sqlserver_df = spark.read.format(\"jdbc\") \\\n",
    "                    .option(\"url\", \"jdbc:sqlserver://<user>:1433;databaseName=rohish_zade\") \\\n",
    "                    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "                    .option(\"dbtable\", \"rohish_zade.exams\") \\\n",
    "                    .option(\"user\", \"<user>\") \\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08bf7d7a-1140-4237-925a-ef5d2441d71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "question_19_reading_data_in_pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
