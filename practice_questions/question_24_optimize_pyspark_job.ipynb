{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25862441-b1ea-46c5-8fca-12cd72331243",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optimize PySpark jobs for performance (tuning configurations, parallelism, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53805ba1-e368-404b-8ab1-f6811fb71970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Techniques for Optimization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5542c04-5099-4434-b24a-2253e72dccdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1. Memory management\n",
    "- Tune executor memory and driver memory based on your data size.\n",
    "- **Example:**\n",
    "  ```python\n",
    "  spark-submit\n",
    "  --executor -memory 4G\n",
    "  --driver-memory 2G\n",
    "  --conf spark.executor.memoryOverhead=512\n",
    "  ```\n",
    "\n",
    "##### 2. Parallelism\n",
    "- Increase the number of partitions to parallelize data processing. \n",
    "- The rule of thumb is to have 2-4 partitions per CPU core.\n",
    "- Set spark.default.parallelism and spark.sql.shuffle.partitions.\n",
    "- Example:\n",
    "    ```python\n",
    "    conf spark.default.parallelism=100\n",
    "    conf spark.sql.shuffle.partitions=100\n",
    "    ```\n",
    "- You can also control partitions with repartition or coalesce functions:\n",
    "- Example:\n",
    "    ```python\n",
    "    # Increase partitions\n",
    "    df = df.repartition(100)\n",
    "\n",
    "    # Decrease partitions\n",
    "    df = df.coalesce(10)\n",
    "    ```\n",
    "\n",
    "##### 3. Caching/Persisting\n",
    "- Use `cache()` or `persist()` for data reuse, but only for intermediate results that are accessed multiple times.\n",
    "- Be careful with memory usage when persisting large datasets.\n",
    "- You can use `persist(StorageLevel.MEMORY_AND_DISK)` if you run out of memory.\n",
    "\n",
    "##### 4. Broadcast Joins:\n",
    "- Use broadcast join for smaller datasets to avoid large shuffle operations.\n",
    "\n",
    "##### 5. Avoiding Shuffles:\n",
    "- Avoid unnecessary shuffles by using reduceByKey instead of groupByKey, and use partitioning wisely.\n",
    "\n",
    "##### 6. Serialization:\n",
    "- Use Kryo serialization instead of Java serialization for faster processing.\n",
    "\n",
    "##### 7. Data Skew Management:\n",
    "- If data is unevenly distributed, partition skew can slow down jobs. You can use salting to handle data skew:\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "question_24_optimize_pyspark_job",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}