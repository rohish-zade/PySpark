{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad4a311e-d58e-49bb-9472-62358212302f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## To cover all the concepts and commands of PySpark for Data Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a15db2a6-5388-40b3-974c-cff3366398ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cbbf68c-4ea6-4005-adac-445fcb02e8b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35adbafa-7aa6-477f-8c7e-5361235fab5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. PySpark Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cba1d070-4d78-47a6-a7b7-b347f7af00d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Starting a Spark Session:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32484937-0d62-49e7-a62f-2a7774abb652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1985111441010120#setting/sparkui/0407-160420-k18p1hx1/driver-3599689750572343465\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1985111441010120#setting/sparkui/0407-160420-k18p1hx1/driver-3599689750572343465\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Starting a Spark Session:\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"App Name\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fe12bd0-2d17-4eb7-abfc-e03b2c2af5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0564828d-c200-4ef2-9171-f283e5254991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reading Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c61a349-bafb-4f79-9e55-ba1430fa6134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------+------+------------+---------+\n|employee_id|        name|department|salary|joining_date| location|\n+-----------+------------+----------+------+------------+---------+\n|        101|Rohan Sharma|        IT| 75000|  2020-05-12|Bangalore|\n|        102|  Priya Iyer|        HR| 65000|  2019-08-25|    Delhi|\n|        103|Rajesh Kumar|   Finance| 80000|  2021-03-15|   Mumbai|\n|        104| Sneha Patil|        IT| 78000|  2018-07-30|     Pune|\n|        105| Amit Sharma| Marketing| 72000|  2022-01-10|Hyderabad|\n|        106|  Ananya Das|        HR| 67000|  2017-11-20|  Kolkata|\n|        107|Vikram Singh|   Finance| 85000|  2023-06-05|  Chennai|\n|        108| Rohit Verma|        IT| 76000|  2020-09-18|Bangalore|\n|        109| Arjun Mehta| Marketing| 73000|  2019-12-11|    Delhi|\n|        110| Rohish Zade|   Finance| 81000|  2016-04-22|   Mumbai|\n+-----------+------------+----------+------+------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "          .option(\"header\", \"true\") \\\n",
    "          .option(\"inferSchema\", \"true\") \\\n",
    "          .load(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/employee_data.csv\")\n",
    "df.show()\n",
    "\n",
    "# you can give the other formats: parquet, json, orc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e1700a-38f0-474e-ad6d-a8a548b28643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV\n",
    "df = spark.read.csv(\"path_to_file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Parquet\n",
    "df = spark.read.parquet(\"path_to_file.parquet\")\n",
    "\n",
    "# JSON\n",
    "df = spark.read.json(\"path_to_file.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49720939-70d8-4720-be20-c28ad6569e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Writing Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa2376b9-6c6f-4720-8181-3665201bf242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.csv(\"output_path.csv\", header=True)\n",
    "df.write.parquet(\"output_path.parquet\")\n",
    "df.write.json(\"output_path.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68da251a-9276-47ab-a05f-94259d0386dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c5a5998-460d-4cbf-980e-0ffd462f9012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22f2ba82-3bfe-4217-b757-629bddccfd26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Show data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a24f0050-677e-4736-a2b2-65428c7b0f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------+------+------------+---------+\n|employee_id|        name|department|salary|joining_date| location|\n+-----------+------------+----------+------+------------+---------+\n|        101|Rohan Sharma|        IT| 75000|  2020-05-12|Bangalore|\n|        102|  Priya Iyer|        HR| 65000|  2019-08-25|    Delhi|\n|        103|Rajesh Kumar|   Finance| 80000|  2021-03-15|   Mumbai|\n|        104| Sneha Patil|        IT| 78000|  2018-07-30|     Pune|\n|        105| Amit Sharma| Marketing| 72000|  2022-01-10|Hyderabad|\n+-----------+------------+----------+------+------------+---------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# show first 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07201b50-18be-4257-bd91-8863cba0fca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- joining_date: date (nullable = true)\n |-- location: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# prints schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5682d61e-659e-4068-b16b-0d68dec94bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: ['employee_id', 'name', 'department', 'salary', 'joining_date', 'location']"
     ]
    }
   ],
   "source": [
    "# list the the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61d5fce8-f340-4d2c-8fa4-e7a31ce531c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+----------+-----------------+---------+\n|summary|       employee_id|        name|department|           salary| location|\n+-------+------------------+------------+----------+-----------------+---------+\n|  count|                10|          10|        10|               10|       10|\n|   mean|             105.5|        null|      null|          75200.0|     null|\n| stddev|3.0276503540974917|        null|      null|6214.677966091424|     null|\n|    min|               101| Amit Sharma|   Finance|            65000|Bangalore|\n|    max|               110|Vikram Singh| Marketing|            85000|     Pune|\n+-------+------------------+------------+----------+-----------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# summary statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7d87c7c-beae-44c0-8677-3a7c03cda4cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Select columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a3c6e8f-5dc5-4804-b85f-ee819dfd448d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n|employee_id|        name|\n+-----------+------------+\n|        101|Rohan Sharma|\n|        102|  Priya Iyer|\n|        103|Rajesh Kumar|\n|        104| Sneha Patil|\n|        105| Amit Sharma|\n|        106|  Ananya Das|\n|        107|Vikram Singh|\n|        108| Rohit Verma|\n|        109| Arjun Mehta|\n|        110| Rohish Zade|\n+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"employee_id\", \"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f3a2db6-0380-4fc1-a55c-97d7f5d1a136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Filter data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d93c20db-7081-482f-8827-20cd553eb100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+------+------------+---------+\n|employee_id|       name|department|salary|joining_date| location|\n+-----------+-----------+----------+------+------------+---------+\n|        105|Amit Sharma| Marketing| 72000|  2022-01-10|Hyderabad|\n+-----------+-----------+----------+------+------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"employee_id\"] == 105).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e728aa38-9829-44d6-913e-0d62519a3fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+------+------------+--------+\n|employee_id|       name|department|salary|joining_date|location|\n+-----------+-----------+----------+------+------------+--------+\n|        110|Rohish Zade|   Finance| 81000|  2016-04-22|  Mumbai|\n+-----------+-----------+----------+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"name\"] == 'Rohish Zade').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c38b73ce-1b18-4289-a4a5-6f17e802b9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Group By and Aggregation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a8d689-805b-4230-a9d7-e2005175325a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|department|total_salary|\n+----------+------------+\n|        HR|      132000|\n|   Finance|      246000|\n| Marketing|      145000|\n|        IT|      229000|\n+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").agg(F.sum(\"salary\").alias(\"total_salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abce4979-b7e3-4902-8d27-087c48f683b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Transformations in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "196807dd-2628-4f8f-9e5c-bda6183f75e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Map and FlatMap:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "740cc88c-27ec-4bcc-b1f2-616c4eecee9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- In PySpark, both map() and flatMap() are transformations that operate on each element of a Resilient Distributed Dataset (RDD) or each row of a DataFrame. \n",
    "- They apply a function you provide to each element/row and return a new RDD/DataFrame. \n",
    "- The key difference lies in how they handle the return values of your function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb14297d-63b2-45c1-88c5-aa47787ec96a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**map():**\n",
    "- `One-to-one transformation:` For each element in the input RDD/DataFrame, the map() function applies your provided function and returns exactly one element in the new RDD/DataFrame.\n",
    "- `Preserves structure (to a degree):` If your function returns a collection (like a list), that entire collection is treated as a single element in the resulting RDD/DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567f84fb-0d26-404d-baf5-3754ee14631b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'world'], ['hi', 'there'], ['spark']]\n"
     ]
    }
   ],
   "source": [
    "data = [\"hello world\", \"hi there\", \"spark\"]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def split_string(s):\n",
    "    return s.split(\" \")\n",
    "\n",
    "mapped_rdd = rdd.map(split_string)\n",
    "print(mapped_rdd.collect())\n",
    "\n",
    "# In this example, map() applied split_string to each string, and each resulting list of words became a single element in the mapped_rdd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79de4f94-df0a-4de8-8339-6248e17de379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**flatMap():**\n",
    "\n",
    "- `One-to-many (or one-to-zero) transformation followed by flattening:` The flatMap() function first applies your provided function to each element. However, the function you provide to flatMap() is expected to return an iterable (like a list, tuple, or sequence) for each input element. flatMap() then flattens the resulting collection of iterables into a single RDD/DataFrame.\n",
    "- `Changes structure:` The number of elements in the resulting RDD/DataFrame from flatMap() can be different from the original. If your function returns a list of multiple items for one input element, those multiple items will become individual elements in the output. If your function returns an empty iterable, that input element will effectively be removed from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f55d82-0076-4bf5-bb7c-d6105224e74a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'hi', 'there', 'spark']\n"
     ]
    }
   ],
   "source": [
    "data = [\"hello world\", \"hi there\", \"spark\"]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def split_string_flat(s):\n",
    "    return s.split(\" \")\n",
    "\n",
    "flat_mapped_rdd = rdd.flatMap(split_string_flat)\n",
    "print(flat_mapped_rdd.collect())\n",
    "\n",
    "# Here, flatMap() also applied the splitting function, but then it flattened the resulting lists, so the flat_mapped_rdd contains individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24228613-ac0f-447e-a660-3d31a76a4e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "question_30",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}