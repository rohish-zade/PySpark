{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac37d75-cbb6-441c-9ce0-35e1b318ed66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handling NULL Values in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b9448a-133a-4b41-ac43-103d594246f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data cleaning is a crucial step in any data processing pipeline, and dealing with `Null` values is a common challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe47353-309f-4f50-8cd7-91fa4d9220e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In PySpark, you have several powerful options to handle `NULLs` effectively, such as:\n",
    "- Dropping Null Values: dropna()\n",
    "- Filling Null Values: Using `fillna()` or `replace()` to substitute missing values.\n",
    "- Applying conditional logic with `when()` and `otherwise()`.\n",
    "- using functions like `isNull()` and `isNotNull()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530da6eb-e2d1-4415-8ec9-385d94f6fb83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|  Name| Age|\n+------+----+\n|Rohish|  30|\n|  Ajit|null|\n|Rajani|  25|\n|  null|  35|\n|   Eve|null|\n+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "data = [\n",
    "    (\"Rohish\", 30),\n",
    "    (\"Ajit\", None),\n",
    "    (\"Rajani\", 25),\n",
    "    (None, 35),\n",
    "    (\"Eve\", None)\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "117442ed-0d78-417d-9aaf-676b6b4232ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Dropping Null Values - `dropna()`**:\n",
    "\n",
    "Remove rows with null values in any or specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a8475b-30c3-4592-a1f0-beccf2319e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n|  Name|Age|\n+------+---+\n|Rohish| 30|\n|Rajani| 25|\n+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Drops rows with any null values\n",
    "cleaned_df = df.dropna()\n",
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49786145-7f39-4338-b9f6-56d6d94d133d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|  Name| Age|\n+------+----+\n|Rohish|  30|\n|  Ajit|null|\n|Rajani|  25|\n|  null|  35|\n|   Eve|null|\n+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Drops rows only if all values are null\n",
    "df.dropna(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb85b1a1-4e8b-4f9e-bce4-5f0b51492cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|  Name| Age|\n+------+----+\n|Rohish|  30|\n|  Ajit|null|\n|Rajani|  25|\n|   Eve|null|\n+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Drops rows where name is null. we can give more columns as well\n",
    "df.dropna(subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b76b4fb-f1ef-423d-83e9-68c72c81a328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2877abe-603e-4f32-b67e-46f13983e0da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Filling Null Values (fillna())** - \n",
    "Replace null values with a specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4793cff9-df88-4e7c-8bfa-0a01c2236b6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n|  Name|Age|\n+------+---+\n|Rohish| 30|\n|  Ajit|  0|\n|Rajani| 25|\n|  null| 35|\n|   Eve|  0|\n+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "filled_df = df.fillna(0)\n",
    "filled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2d2471-5870-4e84-b749-5273b02f28f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n|  Name|Age|\n+------+---+\n|Rohish| 30|\n|  Ajit|  0|\n|Rajani| 25|\n|  null| 35|\n|   Eve|  0|\n+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "filled_df2 = df.fillna(0, subset=[\"Name\", \"Age\"])\n",
    "filled_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1dc3f7d-8ab3-4759-9975-8d2876e731a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Replacing Null Values (na.replace()):**\n",
    "Similar to fillna(), but more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5043f5-0985-4899-a40c-83b64755edfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n|   Name| Age|\n+-------+----+\n| Rohish|  30|\n|   Ajit|null|\n| Rajani|  25|\n|Unknown|  35|\n|    Eve|null|\n+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"Unknown\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949d11a5-e9de-4312-9a7d-d2444ef6c91d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note: The behavior youâ€™re seeing occurs because `df.na.fill(\"Unknown\")` replaces null values in only `string-type` columns by default. If you want to apply it to all columns (regardless of type), specify subset=None explicitly or provide a list of column names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4799f0b5-4557-43da-9a76-b63a7c7670e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n|   Name| Age|\n+-------+----+\n| Rohish|  30|\n|   Ajit|null|\n| Rajani|  25|\n|Unknown|  35|\n|    Eve|null|\n+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"Unknown\", subset=None).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "944df101-ace0-47b1-9831-8664c3718466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Using coalesce()** Provide a default value if a column has nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a332023-d644-42be-97c0-2bb1141b2727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n|   Name|    Age|\n+-------+-------+\n| Rohish|     30|\n|   Ajit|Unknown|\n| Rajani|     25|\n|Unknows|     35|\n|    Eve|Unknown|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "df_1 = df.withColumn(\"Name\", coalesce(df[\"Name\"], lit(\"Unknows\"))) \\\n",
    "    .withColumn(\"Age\", coalesce(df[\"Age\"], lit(\"Unknown\")))\n",
    "    \n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3b5167-a75d-41e2-9cc1-aba11defbbe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Filtering Out Nulls (filter() or where()):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9353d451-c6ba-42c7-97ab-033157fa7ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|  Name| Age|\n+------+----+\n|Rohish|  30|\n|  Ajit|null|\n|Rajani|  25|\n|   Eve|null|\n+------+----+\n\n+------+----+\n|  Name| Age|\n+------+----+\n|Rohish|  30|\n|  Ajit|null|\n|Rajani|  25|\n|   Eve|null|\n+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"Name\").isNotNull()).show()\n",
    "df.where(col(\"Name\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d50e77c-9f54-4281-9d0b-a17ba664040a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n|Name|Age|\n+----+---+\n|null| 35|\n+----+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"Name\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977bbdcf-898a-438e-a244-de3155ad542a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Using Conditional Logic (when and otherwise):** Apply conditional logic to replace NULL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8a380a-74af-494b-94aa-3fb3ce5e876c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n|   Name|    Age|\n+-------+-------+\n| Rohish|     30|\n|   Ajit|Unknown|\n| Rajani|     25|\n|Unknown|     35|\n|    Eve|Unknown|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df.withColumn(\"Name\", when(col(\"Name\").isNull(), \"Unknown\").otherwise(col(\"Name\"))) \\\n",
    "    .withColumn(\"Age\", when(col(\"Age\").isNull(), \"Unknown\").otherwise(col(\"Age\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0ae3e6d-7480-4fa4-8532-f0a05d870f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee13b31f-d288-41fc-a733-4c3741d6556d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Potential Interview questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebdb54f3-5a65-4861-9679-5c3bde678143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How do you handle NULL values in a PySpark DataFrame?**\n",
    "\n",
    "**Answer:** You can handle NULL values using various functions:\n",
    " - df.fillna(value) to replace NULL values with a specified value.\n",
    " - df.dropna() to drop rows with NULL values.\n",
    " - df.replace(to_replace, value) to replace specific NULL values with another value.\n",
    " - df.na.fill(value) is an alternative way to fill NULL values.\n",
    " - df.na.drop() to remove rows with NULL values.\n",
    " - Using coalesce() Function: Use the first non-null value from a list of columns.\n",
    " - when() and otherwise() Functions: Apply conditional logic to replace NULL values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08b0a8e9-432e-4ed1-8fb3-d494f34a0ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f567239c-805e-4621-9c56-c809a03c1949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How can you filter out rows with NULL values in a specific column?**\n",
    "\n",
    "**Answer:**\n",
    "- We can use the `filter()` or `where()` method with `isNull()` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be40708c-fb03-472c-a583-66bb01cd1a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n|Name| Age|\n+----+----+\n|Ajit|null|\n| Eve|null|\n+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"Age\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3997e81a-e65c-4d9c-a738-67ff36149395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "026f2f5d-cf9d-43a3-9ec9-4b8fd63fa577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How do you handle NULL values when performing aggregations such as SUM, AVG, or COUNT?**\n",
    "\n",
    "**Answer:** PySpark functions like `SUM`, `AVG`, and `COUNT` automatically ignore NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710fd6e6-8d04-43e0-9a6c-3441305f95f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_value: 30.0\nsum_value: 90\n"
     ]
    }
   ],
   "source": [
    "# For example:\n",
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "avg_value = df.select(avg('Age')).collect()[0][0]\n",
    "sum_value = df.select(sum('Age')).collect()[0][0]\n",
    "\n",
    "print(\"avg_value:\", avg_value)\n",
    "print(\"sum_value:\", sum_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d003c60e-6329-4066-b0ea-1dc7d0a0f0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How can you create a custom function to handle NULL values in PySpark?**\n",
    "\n",
    "**Answer:** we can use the udf (User Defined Function) feature and we can utilize functions like fillna to handle null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06464426-03cd-49f6-8f96-86ca7d7ba7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    " from pyspark.sql.types import StringType\n",
    "\n",
    " def replace_null(value):\n",
    " return 0 if value is None else value\n",
    "\n",
    " replace_null_udf = udf(replace_null, IntegerType())\n",
    " df_filled = df.withColumn('column_name', replace_null_udf(df['column_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06173a6d-c52b-45f1-81bc-fb0986641bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|  Name| Age|\n+------+----+\n|Rohish|  30|\n|  Ajit|null|\n|Rajani|  25|\n|     0|  35|\n|   Eve|null|\n+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# for example\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def replace_null(value):\n",
    "    return 0 if value is None else value\n",
    "\n",
    "# Registering the Function as a UDF\n",
    "replace_null_udf =  udf(replace_null, StringType())\n",
    "\n",
    "filled_df = df.withColumn(\"Name\", replace_null_udf(df[\"Name\"]))\n",
    "\n",
    "filled_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "15_handling_null_values_in_pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
