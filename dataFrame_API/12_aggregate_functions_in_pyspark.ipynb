{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fb1ddfb-a95e-4360-ae44-81a7581b2cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Aggregate functions in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff8642f0-efdf-4849-a676-a171822e1b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aggregate functions in PySpark are functions that operate on a group of rows and return a single value.\n",
    "\n",
    "These functions are available in the `pyspark.sql.functions` module and can be used with methods like `groupBy()`, `agg()`, or directly on the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9db73c0d-2364-4e12-a0da-08647f52d8b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Common Aggregate Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38a895ee-15b1-4c26-9349-85a97c56ae18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| **Function**               | **Description**                                                     |\n",
    "|----------------------------|---------------------------------------------------------------------|\n",
    "| **`count()`**              | Counts the number of rows.                                         |\n",
    "| **`sum()`**                | Computes the sum of a numeric column.                              |\n",
    "| **`avg()`**                | Computes the average (mean) of a numeric column.                   |\n",
    "| **`min()`**                | Returns the minimum value in a column.                             |\n",
    "| **`max()`**                | Returns the maximum value in a column.                             |\n",
    "| **`collect_list()`**       | Aggregates values into a list for each group.                      |\n",
    "| **`collect_set()`**        | Aggregates unique values into a set for each group.                |\n",
    "| **`first()`**              | Returns the first value in a group.                                |\n",
    "| **`last()`**               | Returns the last value in a group.                                 |\n",
    "| **`stddev()`**             | Computes the standard deviation of a numeric column.               |\n",
    "| **`variance()`**           | Computes the variance of a numeric column.                         |\n",
    "| **`approx_count_distinct()`** | Approximates the count of distinct values in a column.          |\n",
    "| **`sumDistinct()`**        | Computes the sum of unique values in a column.                     |\n",
    "| **`skewness()`**           | Computes the skewness of a numeric column.                         |\n",
    "| **`kurtosis()`**           | Computes the kurtosis of a numeric column.                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2a87926-7151-4cb2-a2b5-dbcc1ade7729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bde21ab-26b9-4ef0-b37a-4d5d7d0000d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n|  id|   name| age|salary|country|       dept|\n+----+-------+----+------+-------+-----------+\n|   1| Rohish|  26| 20000|  india|         IT|\n|   2| Melody|null| 40000|germany|engineering|\n|   3|  Pawan|  12| 60000|  india|      sales|\n|   4|Roshini|  44|  null|     uk|engineering|\n|   5|Raushan|  35| 70000|  india|      sales|\n|   6|   null|  29|200000|     uk|         IT|\n|   7|   Adam|  37| 65000|     us|         IT|\n|   8|  Chris|  16| 40000|     us|      sales|\n|null|   null|null|  null|   null|       null|\n|   7|   Adam|  37| 65000|     us|         IT|\n+----+-------+----+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max, count\n",
    "\n",
    "# sample data\n",
    "data = [\n",
    "(1,'Rohish',26,20000,'india','IT'),\n",
    "(2,'Melody',None,40000,'germany','engineering'),\n",
    "(3,'Pawan',12,60000,'india','sales'),\n",
    "(4,'Roshini',44,None,'uk','engineering'),\n",
    "(5,'Raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'Adam',37,65000,'us','IT'),\n",
    "(8,'Chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'Adam',37,65000,'us','IT')]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"salary\", \"country\", \"dept\"]\n",
    "\n",
    "emp_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1caae40c-9efd-4c8c-9483-567cac556239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### count():\n",
    "- `count()` in PySpark works as both transformation and action\n",
    "- When you do count on df, it will return df count its works as action but when you do count on any columns it return a new df, works as transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32fddb18-6855-4386-a4da-7f97b9b8d215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: 10"
     ]
    }
   ],
   "source": [
    "# count() as action\n",
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2f9ae1-b8af-45ae-86d3-87638e4ad147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: DataFrame[count(id): bigint]"
     ]
    }
   ],
   "source": [
    "# count() as transformation: its returning a new dataframe\n",
    "emp_df.select(count(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d55e081-4e39-4a2b-b76f-2898329c5ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|count(id)|\n+---------+\n|        9|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# saving the result to a new dataframe and displaying that df\n",
    "id_count_df = emp_df.select(count(\"id\")) # if you are selecting only one column it wont count the null\n",
    "id_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2367eee0-dc9c-4de0-a61c-b6bb0ca857da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### sum, min, max, and avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25e08e34-051b-427f-a254-779aa27af7fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Basic Aggregation without Grouping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38857153-90ef-404a-84d2-af23dc86c463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------+\n|min_salary|max_salary|avg_salary|total_salary|\n+----------+----------+----------+------------+\n|     20000|    200000|   70000.0|      560000|\n+----------+----------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(\n",
    "  min(\"salary\").alias(\"min_salary\"),\n",
    "  max(\"salary\").alias(\"max_salary\"),\n",
    "  avg(\"salary\").alias(\"avg_salary\"),\n",
    "  sum(\"salary\").alias(\"total_salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec535301-cbea-40ea-b0c1-3d8cfda877b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------+\n|min_salary|max_salary|avg_salary|total_salary|\n+----------+----------+----------+------------+\n|     20000|    200000|     70000|      560000|\n+----------+----------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# casting the result\n",
    "emp_df.select(\n",
    "  min(\"salary\").alias(\"min_salary\").cast(\"int\"),\n",
    "  max(\"salary\").alias(\"max_salary\").cast(\"int\"),\n",
    "  avg(\"salary\").alias(\"avg_salary\").cast(\"int\"),\n",
    "  sum(\"salary\").alias(\"total_salary\").cast(\"int\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a4d458a-e36a-42e0-a131-f489a379677f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41436152-2d67-4f6c-9867-590795223401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Group By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "546fa685-91f3-434a-83fc-42574b08d7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Aggregation with Grouping\n",
    "- You can use groupBy() to group the data by one or more columns and apply aggregate functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9412876c-d42f-4ea8-b030-a205b0d5dbdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|       dept|count|\n+-----------+-----+\n|         IT|    4|\n|engineering|    2|\n|      sales|    3|\n|       null|    1|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.groupBy(\"dept\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a48996-4d8e-4cfc-82ef-c273d885c378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n|       dept|dept_wise_salary|\n+-----------+----------------+\n|         IT|          350000|\n|engineering|           40000|\n|      sales|          170000|\n|       null|            null|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# dept wise salary\n",
    "emp_df.groupBy(\"dept\").agg(sum(\"salary\").alias(\"dept_wise_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b72f95c-4274-43ee-953b-d8eea54b0a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------------+\n|country|       dept|country_wise_dept_salary|\n+-------+-----------+------------------------+\n|   null|       null|                    null|\n|germany|engineering|                   40000|\n|  india|         IT|                   20000|\n|  india|      sales|                  130000|\n|     uk|engineering|                    null|\n|     uk|         IT|                  200000|\n|     us|         IT|                  130000|\n|     us|      sales|                   40000|\n+-------+-----------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# country wise dept salary\n",
    "emp_df.groupBy(\"country\",\"dept\").agg(sum(\"salary\").alias(\"country_wise_dept_salary\")).sort(\"country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df7b7fd-4973-44f6-9a81-dbf4a006b127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------+\n|       dept|Total_Salary|Employee_Count|\n+-----------+------------+--------------+\n|         IT|      350000|             4|\n|engineering|       40000|             2|\n|      sales|      170000|             3|\n|       null|        null|             0|\n+-----------+------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Group by Department and aggregate\n",
    "emp_df.groupBy(\"dept\").agg(\n",
    "    sum(\"salary\").alias(\"Total_Salary\"),\n",
    "    count(\"id\").alias(\"Employee_Count\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d05842cd-5e23-41ba-b4ae-73d8b77a7eed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Using Multiple Aggregates in One Step:**\n",
    "\n",
    "You can use `agg()` for multiple aggregate functions in a single statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "561a30ae-56ee-42a5-aa5c-98c3854a5926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------+----------+----------+\n|       dept|total_salary|avg_salary|min_salary|max_salary|\n+-----------+------------+----------+----------+----------+\n|         IT|      350000|     87500|     20000|    200000|\n|engineering|       40000|     40000|     40000|     40000|\n|      sales|      170000|     56666|     40000|     70000|\n|       null|        null|      null|      null|      null|\n+-----------+------------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.groupBy(\"dept\").agg(\n",
    "  sum(\"Salary\").alias(\"total_salary\"),\n",
    "  avg(\"Salary\").cast(\"int\").alias(\"avg_salary\"),\n",
    "  min(\"Salary\").alias(\"min_salary\"),\n",
    "  max(\"Salary\").alias(\"max_salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c8df36-5e33-48b9-b3c3-22150168c4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "aggregate_functions_in_pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
