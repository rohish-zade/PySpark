{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8cd50d8-fe20-47a5-9db1-ef1979c5b7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unique & Sorted records In PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9583480-113a-4b15-9b3e-cf488ab58128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Unique records in PySpark\n",
    "To extract unique records in PySpark, you can use the distinct() function on a DataFrame. It removes duplicate rows, giving you only the unique records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32aa1d42-55f4-4018-bee9-8915adf25390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1413c72d-b88f-4252-80b5-9fd6df9a9d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| id|  name|salary|age|\n+---+------+------+---+\n| 10|Rohish| 50000| 18|\n| 11| Vikas| 75000| 16|\n| 12| Nisha| 40000| 18|\n| 13| Nidhi| 60000| 17|\n| 14| Priya| 80000| 18|\n| 15| Mohit| 45000| 18|\n| 16|Rajesh| 90000| 10|\n| 17| Raman| 55000| 16|\n| 18|   Sam| 65000| 17|\n| 15| Mohit| 45000| 18|\n| 13| Nidhi| 60000| 17|\n| 14| Priya| 90000| 18|\n| 18|   Sam| 65000| 17|\n+---+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data=[(10 ,'Rohish',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(13 ,'Nidhi',60000,  17),      \n",
    "(14 ,'Priya',90000,  18),  \n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "columns = [\"id\", \"name\", \"salary\", \"age\"]\n",
    "\n",
    "emp_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c914f5-8459-4b79-9cef-6a886984dc3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: 13"
     ]
    }
   ],
   "source": [
    "# without distinct()\n",
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f0b5ec-c392-4ece-920e-788c528866b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: 10"
     ]
    }
   ],
   "source": [
    "# without distinct()\n",
    "# emp_df.distinct().show()\n",
    "emp_df.select(\"*\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8ae64b-aee2-4614-b1fd-94b9d087c228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| id|  name|salary|age|\n+---+------+------+---+\n| 10|Rohish| 50000| 18|\n| 12| Nisha| 40000| 18|\n| 11| Vikas| 75000| 16|\n| 13| Nidhi| 60000| 17|\n| 15| Mohit| 45000| 18|\n| 14| Priya| 80000| 18|\n| 16|Rajesh| 90000| 10|\n| 17| Raman| 55000| 16|\n| 18|   Sam| 65000| 17|\n| 14| Priya| 90000| 18|\n+---+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "856d32b8-9ce9-448b-8e24-18bb6fa28e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Key Notes:**\n",
    "- `distinct()` works on the entire row, so even if one column differs, it considers the row as unique.Key Notes:\n",
    "- For selecting unique rows based on specific columns, you can use `dropDuplicates()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64a46b31-1a65-4f6e-bbdb-c8db0b1ebe4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**dropDuplicates():**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766aa3bc-aacf-45ab-acb3-49c9a9b98f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| id|  name|salary|age|\n+---+------+------+---+\n| 10|Rohish| 50000| 18|\n| 11| Vikas| 75000| 16|\n| 12| Nisha| 40000| 18|\n| 13| Nidhi| 60000| 17|\n| 15| Mohit| 45000| 18|\n| 14| Priya| 80000| 18|\n| 17| Raman| 55000| 16|\n| 16|Rajesh| 90000| 10|\n| 18|   Sam| 65000| 17|\n+---+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Get unique records based on specific columns\n",
    "emp_df.dropDuplicates([\"id\", \"name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4ed14c8-7b11-4979-8da9-ae3dc3fe35fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca18f2f-0504-4d9a-b200-f01a2d613c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sorting In PySpark\n",
    "- In PySpark, you can sort a DataFrame using the `sort()` or `orderBy()` methods. \n",
    "- Both methods work similarly, allowing you to sort the data either in ascending or descending order, and by one or multiple columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "340c71a5-4fe1-4bce-a01b-4a74f72a19be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Syntax for Sorting:**\n",
    "- `Ascending Order (default): `df.sort(\"column_name\")\n",
    "- `Descending Order:` df.sort(col(\"column_name\").desc())\n",
    "- `Multiple Columns: `df.sort(\"col1\", \"col2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189ae8dc-1c4c-4828-8e03-afc2f9475f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| id|  name|salary|age|\n+---+------+------+---+\n| 10|Rohish| 50000| 18|\n| 11| Vikas| 75000| 16|\n| 12| Nisha| 40000| 18|\n| 13| Nidhi| 60000| 17|\n| 13| Nidhi| 60000| 17|\n| 14| Priya| 90000| 18|\n| 14| Priya| 80000| 18|\n| 15| Mohit| 45000| 18|\n| 15| Mohit| 45000| 18|\n| 16|Rajesh| 90000| 10|\n| 17| Raman| 55000| 16|\n| 18|   Sam| 65000| 17|\n| 18|   Sam| 65000| 17|\n+---+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# sort\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "emp_df.sort(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b7d6d9-fda7-441f-abe8-c6c0f2b8bf9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| id|  name|salary|age|\n+---+------+------+---+\n| 10|Rohish| 50000| 18|\n| 11| Vikas| 75000| 16|\n| 12| Nisha| 40000| 18|\n| 13| Nidhi| 60000| 17|\n| 13| Nidhi| 60000| 17|\n| 14| Priya| 90000| 18|\n| 14| Priya| 80000| 18|\n| 15| Mohit| 45000| 18|\n| 15| Mohit| 45000| 18|\n| 16|Rajesh| 90000| 10|\n| 17| Raman| 55000| 16|\n| 18|   Sam| 65000| 17|\n| 18|   Sam| 65000| 17|\n+---+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# orderBy\n",
    "emp_df.orderBy(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36bc3163-d1a9-495b-ad72-85e17d31e9cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**orderBy() is an alias for sort(), and the two can be used interchangeably and their performance is the same.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab8d6f2-9475-4de7-a250-73a821150b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Examples:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80862872-a5f7-4d2a-9ef3-b92f49481c5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sorting in Ascending Order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f451483c-868f-4b65-93d5-596a833b13a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| id|  name|salary|age|\n+---+------+------+---+\n| 15| Mohit| 45000| 18|\n| 15| Mohit| 45000| 18|\n| 13| Nidhi| 60000| 17|\n| 13| Nidhi| 60000| 17|\n| 12| Nisha| 40000| 18|\n| 14| Priya| 90000| 18|\n| 14| Priya| 80000| 18|\n| 16|Rajesh| 90000| 10|\n| 17| Raman| 55000| 16|\n| 10|Rohish| 50000| 18|\n| 18|   Sam| 65000| 17|\n| 18|   Sam| 65000| 17|\n| 11| Vikas| 75000| 16|\n+---+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.sort(col(\"name\").asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbf3292-c54a-4d3d-b42f-7400d7fdfbed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sorting in Descending  Order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe80c27-2c67-4d42-aa94-fb14b9805f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| id|  name|salary|age|\n+---+------+------+---+\n| 11| Vikas| 75000| 16|\n| 18|   Sam| 65000| 17|\n| 18|   Sam| 65000| 17|\n| 10|Rohish| 50000| 18|\n| 17| Raman| 55000| 16|\n| 16|Rajesh| 90000| 10|\n| 14| Priya| 80000| 18|\n| 14| Priya| 90000| 18|\n| 12| Nisha| 40000| 18|\n| 13| Nidhi| 60000| 17|\n| 13| Nidhi| 60000| 17|\n| 15| Mohit| 45000| 18|\n| 15| Mohit| 45000| 18|\n+---+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.sort(col(\"name\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c14ee4da-ee67-46c5-b85e-c0a33d16f44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sorting by Multiple Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e91210b-1172-48c3-81a5-a762fc438636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| id|  name|salary|age|\n+---+------+------+---+\n| 15| Mohit| 45000| 18|\n| 15| Mohit| 45000| 18|\n| 13| Nidhi| 60000| 17|\n| 13| Nidhi| 60000| 17|\n| 12| Nisha| 40000| 18|\n| 14| Priya| 90000| 18|\n| 14| Priya| 80000| 18|\n| 16|Rajesh| 90000| 10|\n| 17| Raman| 55000| 16|\n| 10|Rohish| 50000| 18|\n| 18|   Sam| 65000| 17|\n| 18|   Sam| 65000| 17|\n| 11| Vikas| 75000| 16|\n+---+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# First by name (Ascending), then by salary (Descending):\n",
    "emp_df.sort(col(\"name\").asc(), col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d816690-28c7-4033-8831-6385dcb0a20e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unique_&_sorted_records_in_pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
