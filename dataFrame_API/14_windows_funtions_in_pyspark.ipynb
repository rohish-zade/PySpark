{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91aebafb-1756-4df7-a4a0-ec28d304cbe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Window functions in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff69665-82db-4812-9968-d655c561164d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Window functions in PySpark are used to perform operations on a subset of data (or a \"window\") defined by one or more columns, without collapsing the dataset into an aggregated result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a544a77f-879b-483c-99fe-c4934a0d31e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Common Types of Window Functions\n",
    "- **Ranking Functions:** E.g., row_number(), rank(), dense_rank()\n",
    "- **Aggregate Functions:** E.g., avg(), sum(), max(), min(), count()\n",
    "- **Analytical Functions:** E.g., lead(), lag(), ntile(), cume_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0f85800-32df-4cfa-a748-09c2a3da1eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Steps to Use Window Functions**\n",
    "- **Import Required Modules**\n",
    "  ```python\n",
    "  from pyspark.sql import SparkSession\n",
    "  from pyspark.sql.window import Window\n",
    "  from pyspark.sql.functions import *\n",
    "\n",
    "  ```\n",
    "- **Define the Window Specification** Use `Window.partitionBy()` to define partitions and `Window.orderBy()` to define the order.\n",
    "- **Apply the Window Function** Use functions like `row_number(),` `rank()`, or aggregate functions with` over()` to apply the window spec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22cf534e-8a42-4731-8431-7a94e0ddda64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58601c96-eac6-46f6-970f-8be86d9495ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+------+\n| id|   name|salary|department|gender|\n+---+-------+------+----------+------+\n|  1| Rohish| 50000|        IT|     m|\n|  2|   Smit| 60000|     sales|     m|\n|  3| Chetan| 70000| marketing|     m|\n|  4| Rajesh| 80000|        IT|     m|\n|  5| Melody| 90000|     sales|     f|\n|  6|  Babli| 45000| marketing|     f|\n|  7|   Aish| 55000| marketing|     f|\n|  8|Ashwini|100000|        IT|     f|\n|  9| Harris| 65000|        IT|     m|\n| 10|  Veeru| 50000| marketing|     m|\n| 11|Sradhya| 50000|        IT|     f|\n| 12| Akshay| 90000|     sales|     m|\n+---+-------+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# sample data\n",
    "emp_data = [(1,'Rohish',50000,'IT','m'),\n",
    "(2,'Smit',60000,'sales','m'),\n",
    "(3,'Chetan',70000,'marketing','m'),\n",
    "(4,'Rajesh',80000,'IT','m'),\n",
    "(5,'Melody',90000,'sales','f'),\n",
    "(6,'Babli',45000,'marketing','f'),\n",
    "(7,'Aish',55000,'marketing','f'),\n",
    "(8,'Ashwini',100000,'IT','f'),\n",
    "(9,'Harris',65000,'IT','m'),\n",
    "(10,'Veeru',50000,'marketing','m'),\n",
    "(11,'Sradhya',50000,'IT','f'),\n",
    "(12,'Akshay',90000,'sales','m')]\n",
    "\n",
    "# schema\n",
    "schema =  \"id int, name string, salary int, department string, gender string\"\n",
    "\n",
    "# employee dataframe\n",
    "emp_df = spark.createDataFrame(emp_data, schema)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb568f1-3ed5-4ca5-bf0a-fa97d0d138ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ranking Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0496eca5-5fb6-425d-9c49-46a6523cb73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**row_number():** Assigns a unique number to each row in a partition based on the specified order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700f9fc6-8fb2-4071-829e-0ba7498cb160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+------+---+\n| id|   name|salary|department|gender| rn|\n+---+-------+------+----------+------+---+\n|  8|Ashwini|100000|        IT|     f|  1|\n|  4| Rajesh| 80000|        IT|     m|  2|\n|  9| Harris| 65000|        IT|     m|  3|\n|  1| Rohish| 50000|        IT|     m|  4|\n| 11|Sradhya| 50000|        IT|     f|  5|\n|  3| Chetan| 70000| marketing|     m|  1|\n|  7|   Aish| 55000| marketing|     f|  2|\n| 10|  Veeru| 50000| marketing|     m|  3|\n|  6|  Babli| 45000| marketing|     f|  4|\n|  5| Melody| 90000|     sales|     f|  1|\n| 12| Akshay| 90000|     sales|     m|  2|\n|  2|   Smit| 60000|     sales|     m|  3|\n+---+-------+------+----------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
    "emp_df.withColumn(\"rn\", row_number().over(window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e525bc1b-b2b8-467f-8cb8-d9ce85937338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**rank():** Assigns ranks to rows with ties, leaving gaps in ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1149659d-6119-491b-95bb-18b1b25f6511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+------+----+\n| id|   name|salary|department|gender|rank|\n+---+-------+------+----------+------+----+\n|  8|Ashwini|100000|        IT|     f|   1|\n|  4| Rajesh| 80000|        IT|     m|   2|\n|  9| Harris| 65000|        IT|     m|   3|\n|  1| Rohish| 50000|        IT|     m|   4|\n| 11|Sradhya| 50000|        IT|     f|   4|\n|  3| Chetan| 70000| marketing|     m|   1|\n|  7|   Aish| 55000| marketing|     f|   2|\n| 10|  Veeru| 50000| marketing|     m|   3|\n|  6|  Babli| 45000| marketing|     f|   4|\n|  5| Melody| 90000|     sales|     f|   1|\n| 12| Akshay| 90000|     sales|     m|   1|\n|  2|   Smit| 60000|     sales|     m|   3|\n+---+-------+------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "emp_df.withColumn(\"rank\", rank().over(window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5da1a754-666e-4b56-9eb8-42f9e00b60c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**dense_rank():** Assigns ranks without gaps in case of ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329088b3-797a-4b97-9789-fd95e9ed86de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+------+----------+\n| id|   name|salary|department|gender|dense_rank|\n+---+-------+------+----------+------+----------+\n|  8|Ashwini|100000|        IT|     f|         1|\n|  4| Rajesh| 80000|        IT|     m|         2|\n|  9| Harris| 65000|        IT|     m|         3|\n|  1| Rohish| 50000|        IT|     m|         4|\n| 11|Sradhya| 50000|        IT|     f|         4|\n|  3| Chetan| 70000| marketing|     m|         1|\n|  7|   Aish| 55000| marketing|     f|         2|\n| 10|  Veeru| 50000| marketing|     m|         3|\n|  6|  Babli| 45000| marketing|     f|         4|\n|  5| Melody| 90000|     sales|     f|         1|\n| 12| Akshay| 90000|     sales|     m|         1|\n|  2|   Smit| 60000|     sales|     m|         2|\n+---+-------+------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"dense_rank\", dense_rank().over(window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a14dd7c-da1f-40c4-932b-7253a0229ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### popular coding questions related to ranking functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db469cc5-04b5-4276-8540-bf5847b5fb04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Retrieve the Nth Highest(top 2) Salary in Each Department**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048ce1a5-9023-4e7b-b760-786299afcf2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+------+----------+\n| id|   name|salary|department|gender|dense_rank|\n+---+-------+------+----------+------+----------+\n|  8|Ashwini|100000|        IT|     f|         1|\n|  4| Rajesh| 80000|        IT|     m|         2|\n|  3| Chetan| 70000| marketing|     m|         1|\n|  7|   Aish| 55000| marketing|     f|         2|\n|  5| Melody| 90000|     sales|     f|         1|\n| 12| Akshay| 90000|     sales|     m|         1|\n|  2|   Smit| 60000|     sales|     m|         2|\n+---+-------+------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "emp_df.withColumn(\"dense_rank\", dense_rank().over(window)) \\\n",
    "      .filter(col(\"dense_rank\") <= 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d99b0e07-ecc9-413b-9d8b-858354934eb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Remove Duplicate Rows Based on a Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001d127c-f068-492f-abfa-c7ca954e43de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+------+\n| id|   name|salary|department|gender|\n+---+-------+------+----------+------+\n|  6|  Babli| 45000| marketing|     f|\n|  1| Rohish| 50000|        IT|     m|\n|  7|   Aish| 55000| marketing|     f|\n|  2|   Smit| 60000|     sales|     m|\n|  9| Harris| 65000|        IT|     m|\n|  3| Chetan| 70000| marketing|     m|\n|  4| Rajesh| 80000|        IT|     m|\n| 12| Akshay| 90000|     sales|     m|\n|  8|Ashwini|100000|        IT|     f|\n+---+-------+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Use row_number() and filter for rows with Row_Number = 1.\n",
    "window = Window.partitionBy(\"salary\").orderBy(\"name\")\n",
    "emp_df.withColumn(\"rn\", row_number().over(window)) \\\n",
    "      .filter(col(\"rn\") == 1) \\\n",
    "      .drop(col(\"rn\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db6e6772-aad1-4b77-86e6-113092626232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Find Employees with the Same Rank Across All Departments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4862b8-e38e-4006-b9b2-8d5f6a70cce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n|department|dns_rnk|count|\n+----------+-------+-----+\n|        IT|      4|    2|\n|     sales|      1|    2|\n+----------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
    "\n",
    "emp_df.withColumn(\"dns_rnk\", dense_rank().over(window)) \\\n",
    "    .groupBy(\"department\", \"dns_rnk\") \\\n",
    "    .agg(count(col(\"dns_rnk\")).alias(\"count\")) \\\n",
    "    .filter(col(\"count\") > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec1dec1f-471f-4e4f-a1b1-0d4d7617def7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Find the Department with the Highest Average Rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dfa9d10-104b-4670-956a-311e83b85144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n|department|           avg_rnk|\n+----------+------------------+\n|     sales|1.3333333333333333|\n| marketing|               2.5|\n|        IT|               2.8|\n+----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Identify the department where employees have the highest average rank based on salary.\n",
    "# Solution:\n",
    "# Use dense_rank() to rank employees within each department.\n",
    "# Calculate the average rank for each department.\n",
    "\n",
    "window = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
    "\n",
    "rank_df = emp_df.withColumn(\"dense_rank\", dense_rank().over(window))\n",
    "\n",
    "rank_df.groupBy(\"department\") \\\n",
    "      .agg(avg(col(\"dense_rank\")).alias(\"avg_rnk\")) \\\n",
    "      .sort(col(\"avg_rnk\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef23e84c-a969-4f1a-b03e-a73e0bc7977d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n|department|           avg_rnk|\n+----------+------------------+\n|     sales|1.3333333333333333|\n| marketing|               2.5|\n|        IT|               2.8|\n+----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Identify the department where employees have the highest average rank based on salary.\n",
    "# Solution:\n",
    "# Use dense_rank() to rank employees within each department.\n",
    "# Calculate the average rank for each department.\n",
    "\n",
    "window = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
    "\n",
    "rank_df = emp_df.withColumn(\"dense_rank\", dense_rank().over(window))\n",
    "\n",
    "rank_df.groupBy(\"department\") \\\n",
    "      .agg(avg(col(\"dense_rank\")).alias(\"avg_rnk\")) \\\n",
    "      .sort(col(\"avg_rnk\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7784cf5b-649e-4b8c-9f74-87f56ab30de7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1fccb34-bd07-4b01-8ff5-bdf786f139f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Aggregate functions\n",
    "Aggregate window functions in PySpark allow you to perform computations like `SUM`, `AVG`, `COUNT`, `MAX`, `MIN`, etc., over a specific window (partition) of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee9b39e-a9cf-4ad3-8557-f30dcca313c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n|employee|department|salary|\n+--------+----------+------+\n|    Abhi|        HR|  4000|\n|    Jaya|        HR|  5000|\n| Charlie|        HR|  4500|\n|  Rohish|        IT|  5500|\n|   Rohan|        IT|  6000|\n|  Ramesh|   Finance|  7000|\n|    Ramu|   Finance|  6500|\n+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame\n",
    "data = [\n",
    "    (\"Abhi\", \"HR\", 4000),\n",
    "    (\"Jaya\", \"HR\", 5000),\n",
    "    (\"Charlie\", \"HR\", 4500),\n",
    "    (\"Rohish\", \"IT\", 5500),\n",
    "    (\"Rohan\", \"IT\", 6000),\n",
    "    (\"Ramesh\", \"Finance\", 7000),\n",
    "    (\"Ramu\", \"Finance\", 6500),\n",
    "]\n",
    "\n",
    "columns = [\"employee\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca0d0384-09e7-4309-b661-98846d046595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Cumulative Sum (Running Total)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d11ea35-0aeb-4819-b515-45b149cb4112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----------+\n|employee|department|salary|runing_sum|\n+--------+----------+------+----------+\n|    Abhi|        HR|  4000|      4000|\n| Charlie|        HR|  4500|      8500|\n|    Jaya|        HR|  5000|     13500|\n|  Rohish|        IT|  5500|     19000|\n|   Rohan|        IT|  6000|     25000|\n|    Ramu|   Finance|  6500|     31500|\n|  Ramesh|   Finance|  7000|     38500|\n+--------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# default window frame is rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "window = Window.orderBy(\"salary\")\n",
    "df.withColumn(\"runing_sum\", sum(\"salary\").over(window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "610149ab-f09f-4081-a686-7f8e90f7dd07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation of Frame Parameters**\n",
    "- `Window.unboundedPreceding:` Includes all rows before the current row in the frame.\n",
    "- `Window.unboundedFollowing:` Includes all rows after the current row in the frame.\n",
    "- `Window.currentRow:` Refers to the current row in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c7fa62-0c1a-4596-ae81-b68b5b116d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----------+\n|employee|department|salary|runing_sum|\n+--------+----------+------+----------+\n|    Ramu|   Finance|  6500|      6500|\n|  Ramesh|   Finance|  7000|     13500|\n|    Abhi|        HR|  4000|      4000|\n| Charlie|        HR|  4500|      8500|\n|    Jaya|        HR|  5000|     13500|\n|  Rohish|        IT|  5500|      5500|\n|   Rohan|        IT|  6000|     11500|\n+--------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# default window frame is rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "window = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"runing_sum\", sum(\"salary\").over(window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dd4d3c0-17f2-4b66-8733-1cf02ffb2d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Average Salary Per Partition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d0adf60-bc37-4a93-8a8a-c20ca0ca611d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----------+\n|employee|department|salary|avg_salary|\n+--------+----------+------+----------+\n|  Ramesh|   Finance|  7000|    6750.0|\n|    Ramu|   Finance|  6500|    6750.0|\n|    Abhi|        HR|  4000|    4500.0|\n|    Jaya|        HR|  5000|    4500.0|\n| Charlie|        HR|  4500|    4500.0|\n|  Rohish|        IT|  5500|    5750.0|\n|   Rohan|        IT|  6000|    5750.0|\n+--------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"department\")\n",
    "\n",
    "df.withColumn(\"avg_salary\", avg(\"salary\").over(window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a4f1e5b-f276-44b2-a702-39b67934db1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Maximum and Minimum Salary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "596cb010-63d8-4a7c-8c8f-4baadfdd6e63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----------+----------+\n|employee|department|salary|max_salary|min_salary|\n+--------+----------+------+----------+----------+\n|  Ramesh|   Finance|  7000|      7000|      6500|\n|    Ramu|   Finance|  6500|      7000|      6500|\n|    Abhi|        HR|  4000|      5000|      4000|\n|    Jaya|        HR|  5000|      5000|      4000|\n| Charlie|        HR|  4500|      5000|      4000|\n|  Rohish|        IT|  5500|      6000|      5500|\n|   Rohan|        IT|  6000|      6000|      5500|\n+--------+----------+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"department\")\n",
    "\n",
    "df.withColumn(\"max_salary\", max(\"salary\").over(window)) \\\n",
    "  .withColumn(\"min_salary\", min(\"salary\").over(window)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09539a7e-b27a-425c-8a46-3e93664f2abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "643b639d-1526-4dec-aa21-4b24ecfa7691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analytical Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ef3c7b8-8c26-47fa-b70a-ccba31d73f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**lead()**\n",
    "- Fetch the value of a column from the next row in the same partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8320426f-b03c-4976-89f1-b8a571a71f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-----------+\n|employee|department|salary|next_salary|\n+--------+----------+------+-----------+\n|    Ramu|   Finance|  6500|       7000|\n|  Ramesh|   Finance|  7000|       null|\n|    Abhi|        HR|  4000|       4500|\n| Charlie|        HR|  4500|       5000|\n|    Jaya|        HR|  5000|       null|\n|  Rohish|        IT|  5500|       6000|\n|   Rohan|        IT|  6000|       null|\n+--------+----------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"department\").orderBy(\"Salary\")\n",
    "\n",
    "df.withColumn(\"next_salary\", lead(\"salary\").over(window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3f81acd-d103-4f2f-8f0e-8758326d10a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**lag()**\n",
    "- Fetch the value of a column from the previous row in the same partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f8a109-45cc-48ad-a92d-19882e47f4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-----------+\n|employee|department|salary|next_salary|\n+--------+----------+------+-----------+\n|    Ramu|   Finance|  6500|       null|\n|  Ramesh|   Finance|  7000|       6500|\n|    Abhi|        HR|  4000|       null|\n| Charlie|        HR|  4500|       4000|\n|    Jaya|        HR|  5000|       4500|\n|  Rohish|        IT|  5500|       null|\n|   Rohan|        IT|  6000|       5500|\n+--------+----------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"department\").orderBy(\"Salary\")\n",
    "\n",
    "df.withColumn(\"next_salary\", lag(\"salary\").over(window)).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "windows_funtions_in_pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
