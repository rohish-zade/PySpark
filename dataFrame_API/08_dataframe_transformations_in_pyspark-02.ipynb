{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fbc5e18-3700-41e5-9d53-1c68d8a6c99e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataframe Transformations in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e95f150-753c-4cb1-afae-6fd01281fb22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Topics covered**\n",
    "- Aliasing\n",
    "- Filter / Where\n",
    "- Literal\n",
    "- Adding columns\n",
    "- Renaming columns\n",
    "- Casting data types\n",
    "- Removing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e1bb90-e602-47cd-8adb-167a3f24d147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n| id|    name|age|salary|     address| nominee|\n+---+--------+---+------+------------+--------+\n|  1|  Rohish| 26| 75000|        Pune|nominee1|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n|  3|  Pritam| 22|150000|   Bangalore|nominee3|\n|  4|Prantosh| 17|200000|     Kolkata|nominee4|\n|  5|  Vikash| 31|300000|        null|nominee5|\n+---+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# creating a dataframe\n",
    "df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/employee_1.csv\")\n",
    "\n",
    "# output of df\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0db93fb9-5140-47eb-9afa-457890440411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "059efb82-219c-4e5b-a533-401710a92f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Aliasing\n",
    "Used to assign a temporary name to a DataFrame or column to make it more readable or to avoid conflicts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272c2e7c-c631-4c11-8ddb-304dbdd83131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n|emp_id|    name|\n+------+--------+\n|     1|  Rohish|\n|     2|  Nikita|\n|     3|  Pritam|\n|     4|Prantosh|\n|     5|  Vikash|\n+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Aliasing Columns\n",
    "df.select(col(\"id\").alias(\"emp_id\"), col(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7288c72-228d-4cf7-b549-129cca9d1cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6b7c16-4fc0-4541-a785-0e914cf0e08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filter / Where:\n",
    "Filters rows based on a condition. Both methods are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07f0339d-64ed-4ddd-bfd1-d52e25d97427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+--------+\n| id|    name|age|salary|address| nominee|\n+---+--------+---+------+-------+--------+\n|  4|Prantosh| 17|200000|Kolkata|nominee4|\n|  5|  Vikash| 31|300000|   null|nominee5|\n+---+--------+---+------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# filter for employees whose salary is more than 150000\n",
    "df.filter(col(\"salary\") > 150000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cafac177-3c35-4622-825d-387c44bc6a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+--------+\n| id|    name|age|salary|address| nominee|\n+---+--------+---+------+-------+--------+\n|  4|Prantosh| 17|200000|Kolkata|nominee4|\n|  5|  Vikash| 31|300000|   null|nominee5|\n+---+--------+---+------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# filter for employees whose salary is more than 150000: using where\n",
    "df.where(col(\"salary\") > 150000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871950aa-13d5-4268-8af1-f3d3662a64f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+--------+\n| id|    name|age|salary|address| nominee|\n+---+--------+---+------+-------+--------+\n|  4|Prantosh| 17|200000|Kolkata|nominee4|\n+---+--------+---+------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# filter for employees whose salary is more than 150000 and age < 18\n",
    "df.filter((col(\"salary\") > 150000) & (col(\"age\") < 18)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13fb400-0515-43b1-9f32-5d11aae8f9d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+--------+\n| id|    name|age|salary|address| nominee|\n+---+--------+---+------+-------+--------+\n|  4|Prantosh| 17|200000|Kolkata|nominee4|\n+---+--------+---+------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "fc = (col(\"salary\") > 150000) & (col(\"age\") < 18)\n",
    "df.filter(fc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78d5130a-3e2c-4e4c-b435-d821f8bbe82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a7f35a2-2ebe-460a-9efa-4b382f562f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Literal:\n",
    "Used to add constant values as columns in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a718d4fc-043e-4eb9-b2c1-f6eb2ca45fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+---------+\n| id|    name|age|salary|     address| nominee|last_name|\n+---+--------+---+------+------------+--------+---------+\n|  1|  Rohish| 26| 75000|        Pune|nominee1|     Zade|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|     Zade|\n|  3|  Pritam| 22|150000|   Bangalore|nominee3|     Zade|\n|  4|Prantosh| 17|200000|     Kolkata|nominee4|     Zade|\n|  5|  Vikash| 31|300000|        null|nominee5|     Zade|\n+---+--------+---+------+------------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# add a column last name to df with literal value as \"Zade\"\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(\"*\", lit(\"Zade\").alias(\"last_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dce0d7b-7ca8-4e43-88c9-a443fcca3f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+----------+\n| id|    name|age|salary|     address| nominee|new_column|\n+---+--------+---+------+------------+--------+----------+\n|  1|  Rohish| 26| 75000|        Pune|nominee1|       100|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|       100|\n|  3|  Pritam| 22|150000|   Bangalore|nominee3|       100|\n|  4|Prantosh| 17|200000|     Kolkata|nominee4|       100|\n|  5|  Vikash| 31|300000|        null|nominee5|       100|\n+---+--------+---+------+------------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_column\", lit(100)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "251bc976-7803-47f6-91f4-16bf42581ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eaf4eb6-0275-4640-9bdc-eb38ad774234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding Columns: `withColumn`\n",
    "We can use `withColumn` method to add column to the dataframe\n",
    "\n",
    "The `withColumn` method in PySpark is used to:\n",
    "- Add a new column to a DataFrame.\n",
    "- Update an existing column by applying transformations or functions.\n",
    "- Perform operations such as type casting, arithmetic, and applying expressions to columns.\n",
    "- Syntax: `DataFrame.withColumn(colName, col)`\n",
    "  - `colName`: The name of the new or existing column you want to add or update.\n",
    "  - `col`: A column expression, such as a transformation or calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb4e019-84a3-40f7-b9d1-34f736f3436e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+-----------+\n| id|    name|age|salary|     address| nominee|middle_name|\n+---+--------+---+------+------------+--------+-----------+\n|  1|  Rohish| 26| 75000|        Pune|nominee1|      Jesus|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|      Jesus|\n|  3|  Pritam| 22|150000|   Bangalore|nominee3|      Jesus|\n|  4|Prantosh| 17|200000|     Kolkata|nominee4|      Jesus|\n|  5|  Vikash| 31|300000|        null|nominee5|      Jesus|\n+---+--------+---+------+------------+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# adding a new column name middle_name\n",
    "df.withColumn(\"middle_name\", lit(\"Jesus\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba96fa4-09d8-4f7b-96ea-393791a32793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f2805f6-29c8-4c2b-9755-a2de869c02a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Renaming Columns: `withColumnRenamed`\n",
    "The `withColumnRenamed` method in PySpark is used to rename an existing column in a DataFrame. \n",
    "\n",
    "It is a straightforward and efficient way to change column names without altering the data.\n",
    "\n",
    "**Syntax:**` DataFrame.withColumnRenamed(existing, new)`\n",
    "- `existing`: The current name of the column.\n",
    "- `new`: The new name to assign to the column.\n",
    "\n",
    "**`Non-Destructive`:** Similar to withColumn, this method returns a new DataFrame without modifying the original DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "728d05ae-dfe6-4ce8-93d7-cb6d33c45334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Renaming a Single Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d205bf67-1d43-4d96-9d83-f1d04bcd224a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+------+------------+--------+\n|emp_id|    name|age|salary|     address| nominee|\n+------+--------+---+------+------------+--------+\n|     1|  Rohish| 26| 75000|        Pune|nominee1|\n|     2|  Nikita| 23|100000|uttarpradesh|nominee2|\n|     3|  Pritam| 22|150000|   Bangalore|nominee3|\n|     4|Prantosh| 17|200000|     Kolkata|nominee4|\n|     5|  Vikash| 31|300000|        null|nominee5|\n+------+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# change the id column name to emp_id\n",
    "df.withColumnRenamed(\"id\", \"emp_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17a3c713-b56f-49db-9fa1-a1e6cfefdb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Rename Multiple Columns (Chained Calls)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f1b319-b96d-4a38-a2b9-7f5756a53c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+------+------------+--------+\n|emp_id|emp_name|age|salary|     address| nominee|\n+------+--------+---+------+------------+--------+\n|     1|  Rohish| 26| 75000|        Pune|nominee1|\n|     2|  Nikita| 23|100000|uttarpradesh|nominee2|\n|     3|  Pritam| 22|150000|   Bangalore|nominee3|\n|     4|Prantosh| 17|200000|     Kolkata|nominee4|\n|     5|  Vikash| 31|300000|        null|nominee5|\n+------+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"id\", \"emp_id\") \\\n",
    "    .withColumnRenamed(\"name\", \"emp_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fcc543d-e6aa-41c7-9bfd-ed9073d2da3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56f40d0c-1a00-4120-af2d-d66b69aacc4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Casting Data Types: Changing the data type of a column\n",
    "\n",
    "PySpark provides the `cast()` method as part of the Column class for changing the data type.\n",
    "\n",
    "**Syntax:** `DataFrame.withColumn(colName, colExpression.cast(dataType))`\n",
    "- `colName`: Name of the column you want to cast.\n",
    "- `colExpression`: The column or transformation to apply.\n",
    "- `dataType`: Target data type (e.g., StringType, IntegerType, etc.).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf981cd0-f9ad-4606-bd67-dde8cd520e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df33ba5b-b287-4926-b7e9-16cbf7ba12a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: long (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# change the datatype of salary to longa\n",
    "df.withColumn(\"salary\", col(\"salary\").cast(\"long\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b987abb-ebdb-489e-9b03-66c5c48fab4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: long (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"salary\", col(\"salary\").cast(\"long\")) \\\n",
    "   .withColumn(\"id\", col(\"id\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14346229-4dcc-4580-b26a-d8868fc4c27a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd86829-23bd-42b7-b827-8d9eed4cc68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dropping Columns:\n",
    "PySpark provides a built-in `drop()` method to remove one or more columns.\n",
    "\n",
    "**Syntax:** `DataFrame.drop(*cols)`\n",
    "- `cols:` The names of the columns to drop (as strings). Multiple column names can be passed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8730a5e0-b935-45a2-a5c6-147161414459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+------------+--------+\n|    name|age|salary|     address| nominee|\n+--------+---+------+------------+--------+\n|  Rohish| 26| 75000|        Pune|nominee1|\n|  Nikita| 23|100000|uttarpradesh|nominee2|\n|  Pritam| 22|150000|   Bangalore|nominee3|\n|Prantosh| 17|200000|     Kolkata|nominee4|\n|  Vikash| 31|300000|        null|nominee5|\n+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Drop a Single Column id\n",
    "df.drop(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69a5794-2b1f-4695-bbfd-a12295e3a62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+\n|    name|     address| nominee|\n+--------+------------+--------+\n|  Rohish|        Pune|nominee1|\n|  Nikita|uttarpradesh|nominee2|\n|  Pritam|   Bangalore|nominee3|\n|Prantosh|     Kolkata|nominee4|\n|  Vikash|        null|nominee5|\n+--------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Drop Multiple Columns: id, age, salary, address\n",
    "df.drop(\"id\", col(\"age\"), df[\"salary\"]).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dataframe_transformations_in_pyspark-02",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
