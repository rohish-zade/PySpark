{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "658606f6-db3c-4082-9186-17f4c1e55334",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How to Read data in PySpark\n",
    "- The `DataFrameReader` API in PySpark is used to load data from various data sources into a DataFrame.\n",
    "- It provides multiple methods for reading data, allowing you to specify the file format, schema, options, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a350406f-6ad2-4eb0-9356-0bfeefd33a08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d5216a-22bc-4755-b514-2f2b32580ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Core Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b5ec614-1114-4798-9f40-0df61ac83181",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "DataFrameReader.format (...) \\\n",
    "    .option (\"key\", \"value\") \\\n",
    "    .schema (...) \\\n",
    "    .load (...)\n",
    "```\n",
    "\n",
    "**format():** \n",
    "- data file format you are reading. (`CSV`, `JSON`, `parquet`, `ORC`, `JBDC/ODBC table`, etc...)\n",
    "\n",
    "**option():** (this is optional) \n",
    "- allows you to specify additional parameters to customize how the data is read.\n",
    "- Common options: we can use multiple option() methods to specify each parameter\n",
    "  - `header`: Whether the file contains a header row. (True or False)\n",
    "  - `delimiter`: Specifies the delimiter used in CSV files, e.g., , or \\t.\n",
    "  - `inferSchema`: Whether Spark should automatically infer the schema from the data (True or False).\n",
    "  - `mode`:\n",
    "\n",
    "**schema():** (this is optional)\n",
    "- manual schema you can pass\n",
    "\n",
    "**load():**\n",
    "- path where our data is residing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d72901c0-ca5f-4505-9d68-9e0a41395861",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7ebce44-c163-44da-9b9f-abb1f8cfd295",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8d05a4d-1e7b-43ab-a2fc-e10d6d2b037a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# reading the csv file using format method\n",
    "flight_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"mode\", \"failfast\") \\\n",
    "    .load(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/2010_summary.csv\")\n",
    "\n",
    "# show method to display the dataframe data\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c925d7-7846-40af-8a42-020bc33ab190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# to see the schema of DF\n",
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f776aacb-10e5-4cf8-8aab-3c44e91fb9ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b89d6e4-2350-4b20-8605-cf1e0d9cbe9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### mode option in the DataFrameReader\n",
    "- The `mode` option in the DataFrameReader API specifies the behavior when Spark encounters `invalid` or `corrupt` data during the data loading process. \n",
    "- It controls how to handle issues such as` missing values`, `malformed records`, or other errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15877549-04ed-4aaf-b756-371e2733f28b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Supported Modes in DataFrameReader:\n",
    "- **permissive (default):**\n",
    "  - This is the default mode. It tries to correct corrupted records and place them in a separate column called _corrupt_record when reading files.\n",
    "  - set null value to corrupted records\n",
    "- **dropMalformed:**\n",
    "  - Drops any rows containing `malformed` or `corrupted` records.\n",
    "  - If a row has an issue (e.g., `incorrect schema`), it will be excluded from the resulting DataFrame.\n",
    "- **failfast:**\n",
    "  - fails execution if malformed `malformed` or `corrupted` records founds in dataset\n",
    "  - Use this mode when you want the job to fail immediately on encountering bad data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc43c02e-e6b6-471f-96e2-f77212c344b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59f1456e-04bd-4045-b5ef-b73abd4c615a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4ff3530-a101-4904-8d0c-eaed20eb538d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating Manual Schema in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdb5587d-dd62-473e-9753-06104687aaf4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Possible interview questions:**\n",
    "\n",
    "- how to create schema in Pyspark?\n",
    "- what are other ways to creating it?\n",
    "- what is structfield and structtype in schema?\n",
    "- what if I have header in my data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca892c79-16f7-44f1-ab6a-aae725ed90df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**In PySpark, you can manually create a schema for a DataFrame in three main ways:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d340fff-871e-4466-bd53-a581800259e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1. Using StructType and StructField:\n",
    "- In PySpark, `StructType` and `StructField` are essential components used to define the schema of a DataFrame.\n",
    "- They are part of PySpark's `pyspark.sql.types` module and are typically used to specify column names, data types, and whether columns can contain null values.\n",
    "- This is the most flexible and commonly used method for defining complex schemas.\n",
    "- **StructField:** represents a single field (or column) within a StructType.\n",
    "- **StructType:** is a list of `StructField` objects, representing the schema of a DataFrame.\n",
    "\n",
    "**Example:** lets create a dataframe for `2010_summary.csv` using manual schema using StructType and StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dffcf71-f350-4f0e-ac7d-5e482b112b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# first we need to import the StructType and StructField\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9378c5f7-2236-4da7-8d72-6ca6928a5340",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = StructType(\n",
    "    [StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "     StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "     StructField(\"count\", IntegerType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a477b4-99fd-4d03-a8c4-5d4af78b3257",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# inferSchema as false\n",
    "flight_df2 = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"mode\", \"failfast\") \\\n",
    "    .load(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/2010_summary.csv\")\n",
    "\n",
    "# When we set inferSchema as false, it took everything as string\n",
    "flight_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd8505d-8d55-4288-a8ce-268b9a6ff4e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Let give the manual schema now\n",
    "flight_df3 = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .schema(my_schema) \\\n",
    "    .option(\"mode\", \"failfast\") \\\n",
    "    .load(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/2010_summary.csv\")\n",
    "\n",
    "flight_df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d609db0a-4ab9-4a30-ac54-f0d7f7b3a2f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "372e8d1d-2df4-403f-a2ce-e2e38226f054",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2. Using a DDL (Data Definition Language):\n",
    "- You can define the schema as a single string with column names and data types, which is useful for simpler schemas.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84cd437a-f200-4268-b904-878e3e77b661",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema_ddl = \"DEST_COUNTRY_NAME string, ORIGIN_COUNTRY_NAME string, count int\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cec817e-1d18-43f0-bfab-7b5768752fbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Let give the manual schema created by DDl\n",
    "flight_df4 = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .schema(my_schema_ddl) \\\n",
    "    .option(\"mode\", \"failfast\") \\\n",
    "    .load(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/2010_summary.csv\")\n",
    "\n",
    "flight_df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50787e43-4d6d-4411-9df6-17266374a50c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1694da8f-ca89-4c4b-97d3-4a944aea0801",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. Using a Python Dictionary (Converted to StructType)\n",
    "- While not directly a schema definition, you can use a dictionary to map column names and types, then convert it to StructType.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ff561e-1a2a-4ae4-a236-c5f8a960cc26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_dict = {\"DEST_COUNTRY_NAME\": StringType(), \"ORIGIN_COUNTRY_NAME\": StringType(), \"count\": IntegerType()}\n",
    "my_schema_dict = StructType([StructField(key, value, True) for key, value in my_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d62420e6-7494-428f-aeb8-95141a116a5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Let give the manual schema created by DDl\n",
    "flight_df5 = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .schema(my_schema_dict) \\\n",
    "    .option(\"mode\", \"failfast\") \\\n",
    "    .load(\"dbfs:/FileStore/shared_uploads/zaderohish5@gmail.com/2010_summary.csv\")\n",
    "\n",
    "flight_df5.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Reading_data_in_spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
