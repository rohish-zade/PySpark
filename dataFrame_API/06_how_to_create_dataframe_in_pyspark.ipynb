{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "411f0f28-e210-40d1-afff-f3394083fe85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How to create dataframe in PySpark\n",
    "In PySpark, you can create a DataFrame using various methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f427c5-cc70-47a7-80e6-40995c11cf61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. From a List of Tuples (Manually Created Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce12b3fa-fe76-472d-b11d-23740819a0da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  1|Rohish|\n|  2| Priya|\n|  3|  Smit|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Sample Data\n",
    "my_data = [(1, \"Rohish\"), (2, \"Priya\"), (3, \"Smit\")]\n",
    "\n",
    "# Define Schema\n",
    "# my_schema = StructType(\n",
    "#     [\n",
    "#         StructField(\"id\", IntegerType(), True),\n",
    "#         StructField(\"name\", StringType(), True)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# also we can give schema in ddl or in list\n",
    "schema1 = \"id integer, name string\"\n",
    "schema2 = [\"id\", \"name\"]\n",
    "\n",
    "# Create DataFrame\n",
    "my_df1 = spark.createDataFrame(data=my_data, schema=schema2)\n",
    "my_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ba5208-47bc-48a7-9188-c073a8001c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1d72c55-ea5a-4932-8093-6282f645ef78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. From a Python Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771b6b5e-9a2f-4633-bb35-62179cc35a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n|Age| ID|  Name|\n+---+---+------+\n| 27|  1|Rohish|\n| 26|  2| Priya|\n| 25|  3|  Smit|\n+---+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [{\"ID\": 1, \"Name\": \"Rohish\", \"Age\": 27},\n",
    "        {\"ID\": 2, \"Name\": \"Priya\", \"Age\": 26},\n",
    "        {\"ID\": 3, \"Name\": \"Smit\", \"Age\": 25}]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8626b113-ee0e-4126-bf5f-b3105feb9141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbd6afb9-8054-4024-b4ae-c904649b4469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Using Pandas DataFrame\n",
    "If you have a Pandas DataFrame, you can convert it to a PySpark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b535f62-0f9f-494e-ba98-6e3abe8a89d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n| ID| Name|Age|\n+---+-----+---+\n|  1|Alice| 25|\n|  2|  Bob| 30|\n|  3|Cathy| 28|\n+---+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create Pandas DataFrame\n",
    "pandas_df = pd.DataFrame({\"ID\": [1, 2, 3], \"Name\": [\"Alice\", \"Bob\", \"Cathy\"], \"Age\": [25, 30, 28]})\n",
    "\n",
    "# Convert to PySpark DataFrame\n",
    "pd_df = spark.createDataFrame(pandas_df)\n",
    "pd_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db87581-0667-44e0-884d-b47929c7761c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Convert Spark to Pandas DataFrame**\n",
    "\n",
    "To convert a PySpark DataFrame to a Pandas DataFrame, you can use the **toPandas()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a19c8f3-66a6-4f93-b091-56f922dc6c39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID   Name  Age\n0   1  Alice   25\n1   2    Bob   30\n2   3  Cathy   28\n"
     ]
    }
   ],
   "source": [
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = pd_df.toPandas()\n",
    "\n",
    "# Display the Pandas DataFrame\n",
    "print(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69b3a80f-2c8a-41fc-aba4-b77067ede3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0adb63-98b9-4ee3-aaa7-2692a0f75b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  From a CSV File\n",
    "```python\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "\n",
    "### From an RDD\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Cathy\", 28)])\n",
    "df = rdd.toDF([\"ID\", \"Name\", \"Age\"])\n",
    "df.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e5307a5-c9f1-4f1f-9841-a506f2dff61a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "how_to_create_dataframe_in_pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
